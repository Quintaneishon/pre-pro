{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d076bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:52:16.610896Z",
     "start_time": "2025-09-17T22:51:40.730347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://ajquintana:****@pypi.artifacts.furycloud.io\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://ajquintana:****@pypi.artifacts.furycloud.io\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://ajquintana:****@pypi.artifacts.furycloud.io\n",
      "Requirement already satisfied: psycopg2-binary in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.9.10)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://ajquintana:****@pypi.artifacts.furycloud.io\n",
      "Requirement already satisfied: sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.0.43)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sqlalchemy) (4.13.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://ajquintana:****@pypi.artifacts.furycloud.io\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install psycopg2-binary\n",
    "!pip install sqlalchemy\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ae761a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Connected to PostgreSQL: PostgreSQL 16.10 (Debian 16.10-1.pgdg13+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit\n",
      "✓ PostgreSQL database initialized with extensions and schemas\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# PostgreSQL connection configuration\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST', 'localhost'),\n",
    "    'port': os.getenv('POSTGRES_PORT', '5432'),\n",
    "    'database': os.getenv('POSTGRES_DB', 'covid_analysis'),\n",
    "    'user': os.getenv('POSTGRES_USER', 'postgres'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'password')\n",
    "}\n",
    "\n",
    "# Helper functions for better code organization\n",
    "def create_database_connections():\n",
    "    \"\"\"Create and return database connections with error handling\"\"\"\n",
    "    try:\n",
    "        # Create SQLAlchemy engine for pandas integration\n",
    "        engine = create_engine(f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\")\n",
    "        \n",
    "        # Direct psycopg2 connection for complex queries\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        conn.autocommit = True\n",
    "        \n",
    "        return engine, conn\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Database connection failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_dataframe(df, required_columns=None, min_rows=1):\n",
    "    \"\"\"Validate DataFrame structure and content\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"DataFrame is empty or None\")\n",
    "    \n",
    "    if len(df) < min_rows:\n",
    "        raise ValueError(f\"DataFrame has fewer than {min_rows} rows\")\n",
    "    \n",
    "    if required_columns:\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Create database connections\n",
    "engine, conn = create_database_connections()\n",
    "\n",
    "def setup_postgresql_database():\n",
    "    \"\"\"Initialize PostgreSQL database with required extensions and schemas\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Enable required extensions with better error handling\n",
    "        extensions = [\n",
    "            \"CREATE EXTENSION IF NOT EXISTS postgres_fdw;\",\n",
    "            \"CREATE EXTENSION IF NOT EXISTS age;\",\n",
    "            \"CREATE EXTENSION IF NOT EXISTS btree_gin;\",\n",
    "            \"CREATE EXTENSION IF NOT EXISTS btree_gist;\"\n",
    "        ]\n",
    "        \n",
    "        for ext_sql in extensions:\n",
    "            try:\n",
    "                cursor.execute(ext_sql)\n",
    "            except Exception as ext_e:\n",
    "                print(f\"⚠ Warning: Could not create extension: {ext_e}\")\n",
    "        \n",
    "        # Create schemas for different data types\n",
    "        schemas = ['relational', 'graph', 'text', 'federation']\n",
    "        for schema in schemas:\n",
    "            cursor.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n",
    "        \n",
    "        print(\"✓ PostgreSQL database initialized with extensions and schemas\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Error setting up database: {e}\")\n",
    "        print(\"Make sure PostgreSQL is running and accessible\")\n",
    "        raise\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Test connection first\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    version = cursor.fetchone()[0]\n",
    "    print(f\"✓ Connected to PostgreSQL: {version}\")\n",
    "    cursor.close()\n",
    "    \n",
    "    # Initialize the database\n",
    "    setup_postgresql_database()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Cannot connect to PostgreSQL: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Make sure Docker is running\")\n",
    "    print(\"2. Start PostgreSQL container: docker-compose up -d\")\n",
    "    print(\"3. Wait for container to be ready: docker-compose logs postgres\")\n",
    "    print(\"4. Check if container is running: docker-compose ps\")\n",
    "    print(\"\\nIf using Docker Compose, the database should be automatically created.\")\n",
    "    print(\"If you're using a local PostgreSQL installation, create the database manually:\")\n",
    "    print(\"  CREATE DATABASE covid_analysis;\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25795d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All dependent objects dropped successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Drop all dependent objects in cascade to clean up the database\"\"\"\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Drop all federation views first (in dependency order)\n",
    "    cursor.execute(\"\"\"\n",
    "    DROP VIEW IF EXISTS federation.comprehensive_correlation CASCADE;\n",
    "    DROP VIEW IF EXISTS federation.unified_covid_data CASCADE;\n",
    "    DROP VIEW IF EXISTS federation.graph_analysis CASCADE;\n",
    "    DROP VIEW IF EXISTS federation.graph_data_extracted CASCADE;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Drop all tables in cascade\n",
    "    cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS relational.covid_cases CASCADE;\n",
    "    DROP TABLE IF EXISTS relational.entidades CASCADE;\n",
    "    DROP TABLE IF EXISTS text.news_articles CASCADE;\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"✓ All dependent objects dropped successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error dropping objects: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe043be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Relational schema created successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create PostgreSQL schema for relational COVID data\"\"\"\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Create main COVID cases table (optimized data types)\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS relational.covid_cases (\n",
    "        id_registro BIGSERIAL PRIMARY KEY,\n",
    "        fecha_actualizacion DATE,\n",
    "        fecha_ingreso DATE,\n",
    "        fecha_sintomas DATE,\n",
    "        fecha_def DATE,\n",
    "        entidad_res BIGINT,\n",
    "        municipio_res BIGINT,\n",
    "        entidad_um BIGINT,\n",
    "        sexo BIGINT,\n",
    "        edad BIGINT,\n",
    "        nacionalidad BIGINT,\n",
    "        embarazo BIGINT,\n",
    "        habla_lengua_indig BIGINT,\n",
    "        indigena BIGINT,\n",
    "        diabetes BIGINT,\n",
    "        obesidad BIGINT,\n",
    "        hipertension BIGINT,\n",
    "        renal_cronica BIGINT,\n",
    "        asma BIGINT,\n",
    "        epoc BIGINT,\n",
    "        cardiovascular BIGINT,\n",
    "        inmusupr BIGINT,\n",
    "        otra_com BIGINT,\n",
    "        cardiovascular_aguda BIGINT,\n",
    "        obesidad_aguda BIGINT,\n",
    "        diabetes_aguda BIGINT,\n",
    "        clasificacion_final BIGINT,\n",
    "        migrante BIGINT,\n",
    "        pais_origen VARCHAR(50),\n",
    "        uci BIGINT,\n",
    "        intubado BIGINT,\n",
    "        neumonia BIGINT,\n",
    "        tipo_paciente BIGINT,\n",
    "        resultado_lab BIGINT,\n",
    "        resultado_antigeno BIGINT,\n",
    "        sector BIGINT,\n",
    "        year INTEGER,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create entity catalog table (optimized data types)\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS relational.entidades (\n",
    "        entidad_id BIGINT PRIMARY KEY,\n",
    "        entidad_nombre VARCHAR(100),\n",
    "        ent_abbr VARCHAR(10)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create indexes for performance\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_covid_cases_entidad ON relational.covid_cases(entidad_res);\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_covid_cases_fecha ON relational.covid_cases(fecha_actualizacion);\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_covid_cases_year ON relational.covid_cases(year);\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_covid_cases_clasificacion ON relational.covid_cases(clasificacion_final);\")\n",
    "    \n",
    "    print(\"✓ Relational schema created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error creating relational schema: {e}\")\n",
    "finally:\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f6fb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded COVID19MEXICO2020.csv: (3868396, 41)\n",
      "  Columns: ['FECHA_ACTUALIZACION', 'ID_REGISTRO', 'ORIGEN', 'SECTOR', 'ENTIDAD_UM', 'SEXO', 'ENTIDAD_NAC', 'ENTIDAD_RES', 'MUNICIPIO_RES', 'TIPO_PACIENTE', 'FECHA_INGRESO', 'FECHA_SINTOMAS', 'FECHA_DEF', 'INTUBADO', 'NEUMONIA', 'EDAD', 'NACIONALIDAD', 'EMBARAZO', 'HABLA_LENGUA_INDIG', 'INDIGENA', 'DIABETES', 'EPOC', 'ASMA', 'INMUSUPR', 'HIPERTENSION', 'OTRA_COM', 'CARDIOVASCULAR', 'OBESIDAD', 'RENAL_CRONICA', 'TABAQUISMO', 'OTRO_CASO', 'TOMA_MUESTRA_LAB', 'RESULTADO_LAB', 'TOMA_MUESTRA_ANTIGENO', 'RESULTADO_ANTIGENO', 'CLASIFICACION_FINAL', 'MIGRANTE', 'PAIS_NACIONALIDAD', 'PAIS_ORIGEN', 'UCI', 'YEAR']\n",
      "✓ Combined relational data: (3868396, 41)\n",
      "✓ Years included: ['2020']\n",
      "✓ All unique columns: ['ASMA', 'CARDIOVASCULAR', 'CLASIFICACION_FINAL', 'DIABETES', 'EDAD', 'EMBARAZO', 'ENTIDAD_NAC', 'ENTIDAD_RES', 'ENTIDAD_UM', 'EPOC', 'FECHA_ACTUALIZACION', 'FECHA_DEF', 'FECHA_INGRESO', 'FECHA_SINTOMAS', 'HABLA_LENGUA_INDIG', 'HIPERTENSION', 'ID_REGISTRO', 'INDIGENA', 'INMUSUPR', 'INTUBADO', 'MIGRANTE', 'MUNICIPIO_RES', 'NACIONALIDAD', 'NEUMONIA', 'OBESIDAD', 'ORIGEN', 'OTRA_COM', 'OTRO_CASO', 'PAIS_NACIONALIDAD', 'PAIS_ORIGEN', 'RENAL_CRONICA', 'RESULTADO_ANTIGENO', 'RESULTADO_LAB', 'SECTOR', 'SEXO', 'TABAQUISMO', 'TIPO_PACIENTE', 'TOMA_MUESTRA_ANTIGENO', 'TOMA_MUESTRA_LAB', 'UCI', 'YEAR']\n",
      "\n",
      "📊 Data Quality Validation:\n",
      "✓ All key columns present: ['CLASIFICACION_FINAL', 'RESULTADO_LAB', 'FECHA_ACTUALIZACION', 'ENTIDAD_RES']\n",
      "✓ Data completeness check:\n",
      "  • CLASIFICACION_FINAL: 0.00% null values\n",
      "  • RESULTADO_LAB: 0.00% null values\n",
      "  • FECHA_ACTUALIZACION: 0.00% null values\n",
      "  • ENTIDAD_RES: 0.00% null values\n",
      "✓ Date range: 2021-10-31 to 2021-10-31\n",
      "✓ Entities represented: 32\n",
      "✓ Records per entity: min=17500, max=1125066\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load and prepare all required datasets with proper column handling for PostgreSQL\"\"\"\n",
    "data_dir = \"../data\"\n",
    "\n",
    "# Load COVID cases data (relational) - all yearly CSV files\n",
    "relational_dir = os.path.join(data_dir, \"relational\")\n",
    "csv_files = [\n",
    "    \"COVID19MEXICO2020.csv\",\n",
    "    # \"COVID19MEXICO2021.csv\", \n",
    "    # \"COVID19MEXICO2022.csv\",\n",
    "    # \"COVID19MEXICO2023.csv\"\n",
    "]\n",
    "\n",
    "df_cases_list = []\n",
    "all_columns = set()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(relational_dir, csv_file)\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            # Fix DtypeWarning by specifying dtype for problematic column\n",
    "            df_year = pd.read_csv(file_path, dtype={'PAIS_ORIGEN': str})\n",
    "            df_year['YEAR'] = csv_file.replace('COVID19MEXICO', '').replace('.csv', '')\n",
    "            df_cases_list.append(df_year)\n",
    "            all_columns.update(df_year.columns)\n",
    "            print(f\"✓ Loaded {csv_file}: {df_year.shape}\")\n",
    "            print(f\"  Columns: {list(df_year.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Error loading {csv_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠ File not found: {csv_file}\")\n",
    "\n",
    "# Combine all yearly data with validation\n",
    "if df_cases_list:\n",
    "    df_cases = pd.concat(df_cases_list, ignore_index=True)\n",
    "    print(f\"✓ Combined relational data: {df_cases.shape}\")\n",
    "    print(f\"✓ Years included: {sorted(df_cases['YEAR'].unique())}\")\n",
    "    print(f\"✓ All unique columns: {sorted(all_columns)}\")\n",
    "    \n",
    "    # Data quality validation\n",
    "    print(\"\\n📊 Data Quality Validation:\")\n",
    "    \n",
    "    # Check for key columns\n",
    "    key_columns = ['CLASIFICACION_FINAL', 'RESULTADO_LAB', 'FECHA_ACTUALIZACION', 'ENTIDAD_RES']\n",
    "    missing_cols = [col for col in key_columns if col not in df_cases.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"⚠ Missing key columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"✓ All key columns present: {key_columns}\")\n",
    "    \n",
    "    # Check data completeness\n",
    "    total_records = len(df_cases)\n",
    "    null_counts = df_cases[key_columns].isnull().sum()\n",
    "    print(f\"✓ Data completeness check:\")\n",
    "    for col in key_columns:\n",
    "        null_pct = (null_counts[col] / total_records) * 100\n",
    "        print(f\"  • {col}: {null_pct:.2f}% null values\")\n",
    "    \n",
    "    # Check date range\n",
    "    if 'FECHA_ACTUALIZACION' in df_cases.columns:\n",
    "        date_col = pd.to_datetime(df_cases['FECHA_ACTUALIZACION'], errors='coerce')\n",
    "        valid_dates = date_col.dropna()\n",
    "        if len(valid_dates) > 0:\n",
    "            print(f\"✓ Date range: {valid_dates.min().date()} to {valid_dates.max().date()}\")\n",
    "        else:\n",
    "            print(\"⚠ No valid dates found in FECHA_ACTUALIZACION\")\n",
    "    \n",
    "    # Check entity distribution\n",
    "    if 'ENTIDAD_RES' in df_cases.columns:\n",
    "        entity_counts = df_cases['ENTIDAD_RES'].value_counts()\n",
    "        print(f\"✓ Entities represented: {len(entity_counts)}\")\n",
    "        print(f\"✓ Records per entity: min={entity_counts.min()}, max={entity_counts.max()}\")\n",
    "        \n",
    "else:\n",
    "    raise ValueError(\"No CSV files could be loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bc1c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading relational data to PostgreSQL...\n",
      "Loading 3868396 rows in chunks of 10000...\n",
      "Loaded chunk 1/387\n",
      "Loaded chunk 2/387\n",
      "Loaded chunk 3/387\n",
      "Loaded chunk 4/387\n",
      "Loaded chunk 5/387\n",
      "Loaded chunk 6/387\n",
      "Loaded chunk 7/387\n",
      "Loaded chunk 8/387\n",
      "Loaded chunk 9/387\n",
      "Loaded chunk 10/387\n",
      "Loaded chunk 11/387\n",
      "Loaded chunk 12/387\n",
      "Loaded chunk 13/387\n",
      "Loaded chunk 14/387\n",
      "Loaded chunk 15/387\n",
      "Loaded chunk 16/387\n",
      "Loaded chunk 17/387\n",
      "Loaded chunk 18/387\n",
      "Loaded chunk 19/387\n",
      "Loaded chunk 20/387\n",
      "Loaded chunk 21/387\n",
      "Loaded chunk 22/387\n",
      "Loaded chunk 23/387\n",
      "Loaded chunk 24/387\n",
      "Loaded chunk 25/387\n",
      "Loaded chunk 26/387\n",
      "Loaded chunk 27/387\n",
      "Loaded chunk 28/387\n",
      "Loaded chunk 29/387\n",
      "Loaded chunk 30/387\n",
      "Loaded chunk 31/387\n",
      "Loaded chunk 32/387\n",
      "Loaded chunk 33/387\n",
      "Loaded chunk 34/387\n",
      "Loaded chunk 35/387\n",
      "Loaded chunk 36/387\n",
      "Loaded chunk 37/387\n",
      "Loaded chunk 38/387\n",
      "Loaded chunk 39/387\n",
      "Loaded chunk 40/387\n",
      "Loaded chunk 41/387\n",
      "Loaded chunk 42/387\n",
      "Loaded chunk 43/387\n",
      "Loaded chunk 44/387\n",
      "Loaded chunk 45/387\n",
      "Loaded chunk 46/387\n",
      "Loaded chunk 47/387\n",
      "Loaded chunk 48/387\n",
      "Loaded chunk 49/387\n",
      "Loaded chunk 50/387\n",
      "Loaded chunk 51/387\n",
      "Loaded chunk 52/387\n",
      "Loaded chunk 53/387\n",
      "Loaded chunk 54/387\n",
      "Loaded chunk 55/387\n",
      "Loaded chunk 56/387\n",
      "Loaded chunk 57/387\n",
      "Loaded chunk 58/387\n",
      "Loaded chunk 59/387\n",
      "Loaded chunk 60/387\n",
      "Loaded chunk 61/387\n",
      "Loaded chunk 62/387\n",
      "Loaded chunk 63/387\n",
      "Loaded chunk 64/387\n",
      "Loaded chunk 65/387\n",
      "Loaded chunk 66/387\n",
      "Loaded chunk 67/387\n",
      "Loaded chunk 68/387\n",
      "Loaded chunk 69/387\n",
      "Loaded chunk 70/387\n",
      "Loaded chunk 71/387\n",
      "Loaded chunk 72/387\n",
      "Loaded chunk 73/387\n",
      "Loaded chunk 74/387\n",
      "Loaded chunk 75/387\n",
      "Loaded chunk 76/387\n",
      "Loaded chunk 77/387\n",
      "Loaded chunk 78/387\n",
      "Loaded chunk 79/387\n",
      "Loaded chunk 80/387\n",
      "Loaded chunk 81/387\n",
      "Loaded chunk 82/387\n",
      "Loaded chunk 83/387\n",
      "Loaded chunk 84/387\n",
      "Loaded chunk 85/387\n",
      "Loaded chunk 86/387\n",
      "Loaded chunk 87/387\n",
      "Loaded chunk 88/387\n",
      "Loaded chunk 89/387\n",
      "Loaded chunk 90/387\n",
      "Loaded chunk 91/387\n",
      "Loaded chunk 92/387\n",
      "Loaded chunk 93/387\n",
      "Loaded chunk 94/387\n",
      "Loaded chunk 95/387\n",
      "Loaded chunk 96/387\n",
      "Loaded chunk 97/387\n",
      "Loaded chunk 98/387\n",
      "Loaded chunk 99/387\n",
      "Loaded chunk 100/387\n",
      "Loaded chunk 101/387\n",
      "Loaded chunk 102/387\n",
      "Loaded chunk 103/387\n",
      "Loaded chunk 104/387\n",
      "Loaded chunk 105/387\n",
      "Loaded chunk 106/387\n",
      "Loaded chunk 107/387\n",
      "Loaded chunk 108/387\n",
      "Loaded chunk 109/387\n",
      "Loaded chunk 110/387\n",
      "Loaded chunk 111/387\n",
      "Loaded chunk 112/387\n",
      "Loaded chunk 113/387\n",
      "Loaded chunk 114/387\n",
      "Loaded chunk 115/387\n",
      "Loaded chunk 116/387\n",
      "Loaded chunk 117/387\n",
      "Loaded chunk 118/387\n",
      "Loaded chunk 119/387\n",
      "Loaded chunk 120/387\n",
      "Loaded chunk 121/387\n",
      "Loaded chunk 122/387\n",
      "Loaded chunk 123/387\n",
      "Loaded chunk 124/387\n",
      "Loaded chunk 125/387\n",
      "Loaded chunk 126/387\n",
      "Loaded chunk 127/387\n",
      "Loaded chunk 128/387\n",
      "Loaded chunk 129/387\n",
      "Loaded chunk 130/387\n",
      "Loaded chunk 131/387\n",
      "Loaded chunk 132/387\n",
      "Loaded chunk 133/387\n",
      "Loaded chunk 134/387\n",
      "Loaded chunk 135/387\n",
      "Loaded chunk 136/387\n",
      "Loaded chunk 137/387\n",
      "Loaded chunk 138/387\n",
      "Loaded chunk 139/387\n",
      "Loaded chunk 140/387\n",
      "Loaded chunk 141/387\n",
      "Loaded chunk 142/387\n",
      "Loaded chunk 143/387\n",
      "Loaded chunk 144/387\n",
      "Loaded chunk 145/387\n",
      "Loaded chunk 146/387\n",
      "Loaded chunk 147/387\n",
      "Loaded chunk 148/387\n",
      "Loaded chunk 149/387\n",
      "Loaded chunk 150/387\n",
      "Loaded chunk 151/387\n",
      "Loaded chunk 152/387\n",
      "Loaded chunk 153/387\n",
      "Loaded chunk 154/387\n",
      "Loaded chunk 155/387\n",
      "Loaded chunk 156/387\n",
      "Loaded chunk 157/387\n",
      "Loaded chunk 158/387\n",
      "Loaded chunk 159/387\n",
      "Loaded chunk 160/387\n",
      "Loaded chunk 161/387\n",
      "Loaded chunk 162/387\n",
      "Loaded chunk 163/387\n",
      "Loaded chunk 164/387\n",
      "Loaded chunk 165/387\n",
      "Loaded chunk 166/387\n",
      "Loaded chunk 167/387\n",
      "Loaded chunk 168/387\n",
      "Loaded chunk 169/387\n",
      "Loaded chunk 170/387\n",
      "Loaded chunk 171/387\n",
      "Loaded chunk 172/387\n",
      "Loaded chunk 173/387\n",
      "Loaded chunk 174/387\n",
      "Loaded chunk 175/387\n",
      "Loaded chunk 176/387\n",
      "Loaded chunk 177/387\n",
      "Loaded chunk 178/387\n",
      "Loaded chunk 179/387\n",
      "Loaded chunk 180/387\n",
      "Loaded chunk 181/387\n",
      "Loaded chunk 182/387\n",
      "Loaded chunk 183/387\n",
      "Loaded chunk 184/387\n",
      "Loaded chunk 185/387\n",
      "Loaded chunk 186/387\n",
      "Loaded chunk 187/387\n",
      "Loaded chunk 188/387\n",
      "Loaded chunk 189/387\n",
      "Loaded chunk 190/387\n",
      "Loaded chunk 191/387\n",
      "Loaded chunk 192/387\n",
      "Loaded chunk 193/387\n",
      "Loaded chunk 194/387\n",
      "Loaded chunk 195/387\n",
      "Loaded chunk 196/387\n",
      "Loaded chunk 197/387\n",
      "Loaded chunk 198/387\n",
      "Loaded chunk 199/387\n",
      "Loaded chunk 200/387\n",
      "Loaded chunk 201/387\n",
      "Loaded chunk 202/387\n",
      "Loaded chunk 203/387\n",
      "Loaded chunk 204/387\n",
      "Loaded chunk 205/387\n",
      "Loaded chunk 206/387\n",
      "Loaded chunk 207/387\n",
      "Loaded chunk 208/387\n",
      "Loaded chunk 209/387\n",
      "Loaded chunk 210/387\n",
      "Loaded chunk 211/387\n",
      "Loaded chunk 212/387\n",
      "Loaded chunk 213/387\n",
      "Loaded chunk 214/387\n",
      "Loaded chunk 215/387\n",
      "Loaded chunk 216/387\n",
      "Loaded chunk 217/387\n",
      "Loaded chunk 218/387\n",
      "Loaded chunk 219/387\n",
      "Loaded chunk 220/387\n",
      "Loaded chunk 221/387\n",
      "Loaded chunk 222/387\n",
      "Loaded chunk 223/387\n",
      "Loaded chunk 224/387\n",
      "Loaded chunk 225/387\n",
      "Loaded chunk 226/387\n",
      "Loaded chunk 227/387\n",
      "Loaded chunk 228/387\n",
      "Loaded chunk 229/387\n",
      "Loaded chunk 230/387\n",
      "Loaded chunk 231/387\n",
      "Loaded chunk 232/387\n",
      "Loaded chunk 233/387\n",
      "Loaded chunk 234/387\n",
      "Loaded chunk 235/387\n",
      "Loaded chunk 236/387\n",
      "Loaded chunk 237/387\n",
      "Loaded chunk 238/387\n",
      "Loaded chunk 239/387\n",
      "Loaded chunk 240/387\n",
      "Loaded chunk 241/387\n",
      "Loaded chunk 242/387\n",
      "Loaded chunk 243/387\n",
      "Loaded chunk 244/387\n",
      "Loaded chunk 245/387\n",
      "Loaded chunk 246/387\n",
      "Loaded chunk 247/387\n",
      "Loaded chunk 248/387\n",
      "Loaded chunk 249/387\n",
      "Loaded chunk 250/387\n",
      "Loaded chunk 251/387\n",
      "Loaded chunk 252/387\n",
      "Loaded chunk 253/387\n",
      "Loaded chunk 254/387\n",
      "Loaded chunk 255/387\n",
      "Loaded chunk 256/387\n",
      "Loaded chunk 257/387\n",
      "Loaded chunk 258/387\n",
      "Loaded chunk 259/387\n",
      "Loaded chunk 260/387\n",
      "Loaded chunk 261/387\n",
      "Loaded chunk 262/387\n",
      "Loaded chunk 263/387\n",
      "Loaded chunk 264/387\n",
      "Loaded chunk 265/387\n",
      "Loaded chunk 266/387\n",
      "Loaded chunk 267/387\n",
      "Loaded chunk 268/387\n",
      "Loaded chunk 269/387\n",
      "Loaded chunk 270/387\n",
      "Loaded chunk 271/387\n",
      "Loaded chunk 272/387\n",
      "Loaded chunk 273/387\n",
      "Loaded chunk 274/387\n",
      "Loaded chunk 275/387\n",
      "Loaded chunk 276/387\n",
      "Loaded chunk 277/387\n",
      "Loaded chunk 278/387\n",
      "Loaded chunk 279/387\n",
      "Loaded chunk 280/387\n",
      "Loaded chunk 281/387\n",
      "Loaded chunk 282/387\n",
      "Loaded chunk 283/387\n",
      "Loaded chunk 284/387\n",
      "Loaded chunk 285/387\n",
      "Loaded chunk 286/387\n",
      "Loaded chunk 287/387\n",
      "Loaded chunk 288/387\n",
      "Loaded chunk 289/387\n",
      "Loaded chunk 290/387\n",
      "Loaded chunk 291/387\n",
      "Loaded chunk 292/387\n",
      "Loaded chunk 293/387\n",
      "Loaded chunk 294/387\n",
      "Loaded chunk 295/387\n",
      "Loaded chunk 296/387\n",
      "Loaded chunk 297/387\n",
      "Loaded chunk 298/387\n",
      "Loaded chunk 299/387\n",
      "Loaded chunk 300/387\n",
      "Loaded chunk 301/387\n",
      "Loaded chunk 302/387\n",
      "Loaded chunk 303/387\n",
      "Loaded chunk 304/387\n",
      "Loaded chunk 305/387\n",
      "Loaded chunk 306/387\n",
      "Loaded chunk 307/387\n",
      "Loaded chunk 308/387\n",
      "Loaded chunk 309/387\n",
      "Loaded chunk 310/387\n",
      "Loaded chunk 311/387\n",
      "Loaded chunk 312/387\n",
      "Loaded chunk 313/387\n",
      "Loaded chunk 314/387\n",
      "Loaded chunk 315/387\n",
      "Loaded chunk 316/387\n",
      "Loaded chunk 317/387\n",
      "Loaded chunk 318/387\n",
      "Loaded chunk 319/387\n",
      "Loaded chunk 320/387\n",
      "Loaded chunk 321/387\n",
      "Loaded chunk 322/387\n",
      "Loaded chunk 323/387\n",
      "Loaded chunk 324/387\n",
      "Loaded chunk 325/387\n",
      "Loaded chunk 326/387\n",
      "Loaded chunk 327/387\n",
      "Loaded chunk 328/387\n",
      "Loaded chunk 329/387\n",
      "Loaded chunk 330/387\n",
      "Loaded chunk 331/387\n",
      "Loaded chunk 332/387\n",
      "Loaded chunk 333/387\n",
      "Loaded chunk 334/387\n",
      "Loaded chunk 335/387\n",
      "Loaded chunk 336/387\n",
      "Loaded chunk 337/387\n",
      "Loaded chunk 338/387\n",
      "Loaded chunk 339/387\n",
      "Loaded chunk 340/387\n",
      "Loaded chunk 341/387\n",
      "Loaded chunk 342/387\n",
      "Loaded chunk 343/387\n",
      "Loaded chunk 344/387\n",
      "Loaded chunk 345/387\n",
      "Loaded chunk 346/387\n",
      "Loaded chunk 347/387\n",
      "Loaded chunk 348/387\n",
      "Loaded chunk 349/387\n",
      "Loaded chunk 350/387\n",
      "Loaded chunk 351/387\n",
      "Loaded chunk 352/387\n",
      "Loaded chunk 353/387\n",
      "Loaded chunk 354/387\n",
      "Loaded chunk 355/387\n",
      "Loaded chunk 356/387\n",
      "Loaded chunk 357/387\n",
      "Loaded chunk 358/387\n",
      "Loaded chunk 359/387\n",
      "Loaded chunk 360/387\n",
      "Loaded chunk 361/387\n",
      "Loaded chunk 362/387\n",
      "Loaded chunk 363/387\n",
      "Loaded chunk 364/387\n",
      "Loaded chunk 365/387\n",
      "Loaded chunk 366/387\n",
      "Loaded chunk 367/387\n",
      "Loaded chunk 368/387\n",
      "Loaded chunk 369/387\n",
      "Loaded chunk 370/387\n",
      "Loaded chunk 371/387\n",
      "Loaded chunk 372/387\n",
      "Loaded chunk 373/387\n",
      "Loaded chunk 374/387\n",
      "Loaded chunk 375/387\n",
      "Loaded chunk 376/387\n",
      "Loaded chunk 377/387\n",
      "Loaded chunk 378/387\n",
      "Loaded chunk 379/387\n",
      "Loaded chunk 380/387\n",
      "Loaded chunk 381/387\n",
      "Loaded chunk 382/387\n",
      "Loaded chunk 383/387\n",
      "Loaded chunk 384/387\n",
      "Loaded chunk 385/387\n",
      "Loaded chunk 386/387\n",
      "Loaded chunk 387/387\n",
      "✓ Successfully loaded 3868396 COVID cases records\n",
      "✓ Loaded 3868396 COVID cases records\n"
     ]
    }
   ],
   "source": [
    "# Optimized data loading functions\n",
    "def load_large_dataframe_to_sql(df, table_name, engine, schema, chunk_size=10000):\n",
    "    \"\"\"Load large DataFrame to PostgreSQL in chunks to avoid memory issues\"\"\"\n",
    "    # Validate input\n",
    "    validate_dataframe(df, min_rows=1)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    print(f\"Loading {total_rows} rows in chunks of {chunk_size}...\")\n",
    "    \n",
    "    try:\n",
    "        # First, create the table structure with the first chunk\n",
    "        first_chunk = df.head(1)\n",
    "        first_chunk.to_sql(table_name, engine, schema=schema, if_exists='replace', index=False)\n",
    "        \n",
    "        # Then insert the remaining data in chunks\n",
    "        total_chunks = (total_rows - 1) // chunk_size + 1\n",
    "        for i in range(1, total_rows, chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            chunk.to_sql(table_name, engine, schema=schema, if_exists='append', index=False)\n",
    "            chunk_num = i // chunk_size + 1\n",
    "            print(f\"Loaded chunk {chunk_num}/{total_chunks}\")\n",
    "        \n",
    "        print(f\"✓ Successfully loaded {total_rows} COVID cases records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def prepare_covid_data(df):\n",
    "    \"\"\"Prepare and clean COVID data for PostgreSQL loading\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert date columns with error handling\n",
    "    date_columns = ['FECHA_ACTUALIZACION', 'FECHA_INGRESO', 'FECHA_SINTOMAS', 'FECHA_DEF']\n",
    "    for col in date_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], format='%Y-%m-%d', errors='coerce')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Load relational data\n",
    "print(\"Loading relational data to PostgreSQL...\")\n",
    "\n",
    "# Prepare COVID cases data for PostgreSQL using helper function\n",
    "df_cases_clean = prepare_covid_data(df_cases)\n",
    "\n",
    "# Replace column names to match PostgreSQL schema\n",
    "column_mapping = {\n",
    "    'ID_REGISTRO': 'id_registro',\n",
    "    'FECHA_ACTUALIZACION': 'fecha_actualizacion',\n",
    "    'FECHA_INGRESO': 'fecha_ingreso',\n",
    "    'FECHA_SINTOMAS': 'fecha_sintomas',\n",
    "    'FECHA_DEF': 'fecha_def',\n",
    "    'ENTIDAD_RES': 'entidad_res',\n",
    "    'MUNICIPIO_RES': 'municipio_res',\n",
    "    'ENTIDAD_UM': 'entidad_um',\n",
    "    'SEXO': 'sexo',\n",
    "    'EDAD': 'edad',\n",
    "    'NACIONALIDAD': 'nacionalidad',\n",
    "    'EMBARAZO': 'embarazo',\n",
    "    'HABLA_LENGUA_INDIG': 'habla_lengua_indig',\n",
    "    'INDIGENA': 'indigena',\n",
    "    'DIABETES': 'diabetes',\n",
    "    'OBESIDAD': 'obesidad',\n",
    "    'HIPERTENSION': 'hipertension',\n",
    "    'RENAL_CRONICA': 'renal_cronica',\n",
    "    'ASMA': 'asma',\n",
    "    'EPOC': 'epoc',\n",
    "    'CARDIOVASCULAR': 'cardiovascular',\n",
    "    'INMUSUPR': 'inmusupr',\n",
    "    'OTRA_COM': 'otra_com',\n",
    "    'CARDIOVASCULAR_AGUDA': 'cardiovascular_aguda',\n",
    "    'OBESIDAD_AGUDA': 'obesidad_aguda',\n",
    "    'DIABETES_AGUDA': 'diabetes_aguda',\n",
    "    'CLASIFICACION_FINAL': 'clasificacion_final',\n",
    "    'MIGRANTE': 'migrante',\n",
    "    'PAIS_ORIGEN': 'pais_origen',\n",
    "    'UCI': 'uci',\n",
    "    'INTUBADO': 'intubado',\n",
    "    'NEUMONIA': 'neumonia',\n",
    "    'TIPO_PACIENTE': 'tipo_paciente',\n",
    "    'RESULTADO_LAB': 'resultado_lab',\n",
    "    'RESULTADO_ANTIGENO': 'resultado_antigeno',\n",
    "    'SECTOR': 'sector',\n",
    "    'YEAR': 'year'\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "df_cases_clean = df_cases_clean.rename(columns=column_mapping)\n",
    "\n",
    "# Load to PostgreSQL\n",
    "load_large_dataframe_to_sql(df_cases_clean, 'covid_cases', engine, 'relational')\n",
    "print(f\"✓ Loaded {len(df_cases_clean)} COVID cases records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1dffc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Graph schema created successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create Apache AGE graph schema for time series data\"\"\"\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Enable AGE library and set path\n",
    "    cursor.execute(\"LOAD 'age';\")\n",
    "    cursor.execute(\"SET search_path = ag_catalog, \\\"$user\\\", public;\")\n",
    "\n",
    "    # Drop existing graph if it exists\n",
    "    cursor.execute(\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "        IF EXISTS (SELECT 1 FROM ag_catalog.ag_graph WHERE name = 'covid_timeseries') THEN\n",
    "            PERFORM drop_graph('covid_timeseries', true);\n",
    "        END IF;\n",
    "    END$$;\n",
    "    \"\"\")\n",
    "\n",
    "    # Create graph for time series data using Apache AGE\n",
    "    cursor.execute(\"SELECT create_graph('covid_timeseries');\")\n",
    "    \n",
    "    # Create nodes for entities (states)\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT create_vlabel('covid_timeseries', 'entidad');\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create nodes for time points\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT create_vlabel('covid_timeseries', 'fecha');\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create edges for time series relationships\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT create_elabel('covid_timeseries', 'tiene_casos');\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"✓ Graph schema created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error creating graph schema: {e}\")\n",
    "    raise\n",
    "    \n",
    "finally:\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca71029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded graph data: 4 series\n"
     ]
    }
   ],
   "source": [
    "# Load catalog data\n",
    "catalogos_file = os.path.join(data_dir, \"relational/240708 Catalogos.xlsx\")\n",
    "cats = pd.read_excel(catalogos_file, sheet_name=None)\n",
    "\n",
    "# Load and transform series data (graph)\n",
    "graph_dir = os.path.join(data_dir, \"graph\")\n",
    "series_files = {\n",
    "    'confirmados': 'Casos_Diarios_Estado_Nacional_Confirmados_20230625.csv',\n",
    "    'defunciones': 'Casos_Diarios_Estado_Nacional_Defunciones_20230625.csv',\n",
    "    'negativos': 'Casos_Diarios_Estado_Nacional_Negativos_20230625.csv',\n",
    "    'sospechosos': 'Casos_Diarios_Estado_Nacional_Sospechosos_20230625.csv'\n",
    "}\n",
    "\n",
    "series_long = {}\n",
    "for name, filename in series_files.items():\n",
    "    file_path = os.path.join(graph_dir, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Transform wide format to long format\n",
    "    id_vars = ['cve_ent', 'poblacion', 'nombre']\n",
    "    date_cols = [col for col in df.columns if col not in id_vars]\n",
    "    \n",
    "    df_long = pd.melt(\n",
    "        df, \n",
    "        id_vars=id_vars, \n",
    "        value_vars=date_cols,\n",
    "        var_name='fecha', \n",
    "        value_name='valor'\n",
    "    )\n",
    "    df_long['metrica'] = name\n",
    "    # FIX: Parse dates with correct format (DD-MM-YYYY)\n",
    "    df_long['fecha'] = pd.to_datetime(\n",
    "        df_long['fecha'].str.strip(),  # remove whitespace\n",
    "        format='%d-%m-%Y',\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Drop invalid dates\n",
    "    df_long = df_long.dropna(subset=['fecha'])    \n",
    "    series_long[name] = df_long\n",
    "\n",
    "print(f\"✓ Loaded graph data: {len(series_long)} series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc57d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 36 entity records\n",
      "Loading time series data to Apache AGE graph...\n",
      "Loading confirmados series data...\n",
      "Loading defunciones series data...\n",
      "Loading negativos series data...\n",
      "Loading sospechosos series data...\n",
      "✓ Time series data loaded to Apache AGE graph\n"
     ]
    }
   ],
   "source": [
    "# Load entity catalog\n",
    "if 'Catálogo de ENTIDADES' in cats:\n",
    "    cat_ent = cats['Catálogo de ENTIDADES'].copy()\n",
    "    cat_ent = cat_ent.rename(columns={\n",
    "        'CLAVE_ENTIDAD': 'entidad_id',\n",
    "        'ENTIDAD_FEDERATIVA': 'entidad_nombre',\n",
    "        'ABREVIATURA': 'ent_abbr'\n",
    "    })\n",
    "    cat_ent.to_sql('entidades', engine, schema='relational', if_exists='replace', index=False)\n",
    "    print(f\"✓ Loaded {len(cat_ent)} entity records\")\n",
    "\n",
    "print(\"Loading time series data to Apache AGE graph...\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    for name, df_series in series_long.items():\n",
    "        print(f\"Loading {name} series data...\")\n",
    "\n",
    "        # Create unique entity nodes\n",
    "        for _, row in df_series.drop_duplicates(['cve_ent', 'nombre', 'poblacion']).iterrows():\n",
    "            cve_ent = str(int(row['cve_ent']))\n",
    "            nombre = row['nombre'].replace(\"'\", \"''\")   # escape quotes\n",
    "            poblacion = int(row['poblacion'])\n",
    "\n",
    "            query = f\"\"\"\n",
    "                SELECT * FROM cypher(\n",
    "                    'covid_timeseries',\n",
    "                    $$\n",
    "                      MERGE (e:entidad {{\n",
    "                        cve_ent: '{cve_ent}',\n",
    "                        nombre: '{nombre}',\n",
    "                        poblacion: {poblacion}\n",
    "                      }})\n",
    "                      RETURN e\n",
    "                    $$\n",
    "                ) AS (e agtype);\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "\n",
    "        # Create date nodes + edges\n",
    "        for _, row in df_series.iterrows():\n",
    "            if pd.notna(row['valor']) and row['valor'] > 0:\n",
    "                cve_ent = str(int(row['cve_ent']))\n",
    "                fecha_str = row['fecha'].strftime('%Y-%m-%d')\n",
    "                valor = int(row['valor'])\n",
    "\n",
    "                query = f\"\"\"\n",
    "                    SELECT * FROM cypher(\n",
    "                        'covid_timeseries',\n",
    "                        $$\n",
    "                          MERGE (f:fecha {{\n",
    "                            fecha: '{fecha_str}',\n",
    "                            metrica: '{name}'\n",
    "                          }})\n",
    "                          WITH f\n",
    "                          MATCH (e:entidad {{cve_ent: '{cve_ent}'}})\n",
    "                          MERGE (e)-[:TIENE_CASOS {{valor: {valor}}}]->(f)\n",
    "                          RETURN e, f\n",
    "                        $$\n",
    "                    ) AS (e agtype, f agtype);\n",
    "                \"\"\"\n",
    "                cursor.execute(query)\n",
    "\n",
    "    print(\"✓ Time series data loaded to Apache AGE graph\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error loading time series data: {e}\")\n",
    "finally:\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3d5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Text schema created successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create JSONB schema for news articles text data\"\"\"\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Create news articles table (instead of Twitter data)\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS text.news_articles (\n",
    "        article_id SERIAL PRIMARY KEY,\n",
    "        title VARCHAR(500),\n",
    "        author VARCHAR(100),\n",
    "        fecha DATE,\n",
    "        categories TEXT[],\n",
    "        content TEXT,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create indexes for performance\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_news_articles_date ON text.news_articles(fecha);\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_news_articles_sentiment ON text.news_articles(categories);\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_news_articles_content ON text.news_articles USING GIN(to_tsvector('spanish', content));\")\n",
    "    \n",
    "    print(\"✓ Text schema created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error creating text schema: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba45d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse news articles from text files\n",
    "text_dir = os.path.join(data_dir, \"text\")\n",
    "text_files = [f for f in os.listdir(text_dir) if f.endswith('.txt')]\n",
    "\n",
    "news_records = []\n",
    "for text_file in text_files:\n",
    "    file_path = os.path.join(text_dir, text_file)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Parse articles from the text file\n",
    "    articles = content.split('=== ARTÍCULO')\n",
    "    \n",
    "    for i, article in enumerate(articles[1:], 1):  # Skip first empty split\n",
    "        # Extract article components\n",
    "        lines = article.strip().split('\\n')\n",
    "        \n",
    "        # Extract title\n",
    "        title_line = lines[0] if lines else \"\"\n",
    "        title = title_line.replace('===', '').strip()\n",
    "        \n",
    "        # Extract metadata\n",
    "        author = \"\"\n",
    "        fecha = \"\"\n",
    "        categories = [] \n",
    "        content_start = 0\n",
    "        \n",
    "        for j, line in enumerate(lines):\n",
    "            if line.startswith('Autor:'):\n",
    "                author = line.replace('Autor:', '').strip()\n",
    "            elif line.startswith('Fecha:'):\n",
    "                fecha = line.replace('Fecha:', '').strip()\n",
    "            elif line.startswith('Categorías:'):\n",
    "                cats_text = line.replace('Categorías:', '').strip()\n",
    "                categories = [cat.strip() for cat in cats_text.split(',')]\n",
    "            elif line.strip() == '--- CONTENIDO ---':\n",
    "                content_start = j + 1\n",
    "                break\n",
    "        \n",
    "        # Extract article content\n",
    "        article_content = '\\n'.join(lines[content_start:]).strip()\n",
    "        \n",
    "        # Parse date\n",
    "        try:\n",
    "            if fecha:\n",
    "                # Handle Spanish month names\n",
    "                fecha_clean = fecha.replace('enero', 'January').replace('febrero', 'February').replace('marzo', 'March').replace('abril', 'April').replace('mayo', 'May').replace('junio', 'June').replace('julio', 'July').replace('agosto', 'August').replace('septiembre', 'September').replace('octubre', 'October').replace('noviembre', 'November').replace('diciembre', 'December')\n",
    "                parsed_date = pd.to_datetime(fecha_clean, errors='coerce')\n",
    "            else:\n",
    "                parsed_date = None\n",
    "        except:\n",
    "            parsed_date = None\n",
    "\n",
    "        news_records.append({\n",
    "            'article_id': f\"{text_file}_{i}\",\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'fecha': parsed_date.date() if parsed_date and pd.notna(parsed_date) else None,\n",
    "            'categories': categories,\n",
    "            'content': article_content,\n",
    "            'source_file': text_file,\n",
    "            'created_at': pd.Timestamp.now()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4587393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 802 news articles\n"
     ]
    }
   ],
   "source": [
    "# Load text data (news articles) to JSONB table\n",
    "if news_records:\n",
    "    df_news_final = pd.DataFrame(news_records)\n",
    "    df_news_final.to_sql('news_articles', engine, schema='text', if_exists='replace', index=False, method='multi')\n",
    "    print(f\"✓ Loaded {len(df_news_final)} news articles\")\n",
    "else:\n",
    "    print(\"⚠ No news articles were parsed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d543a449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Federation schema created successfully with graph integration\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create unified views across schemas including graph data\"\"\"\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # 1. Extract Graph Data (optimized: remove unnecessary casts)\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW federation.graph_data_extracted AS\n",
    "    SELECT \n",
    "        entidad_id::text AS entidad_id,\n",
    "        entidad_nombre::text AS entidad_nombre,\n",
    "        fecha::text::date AS fecha,\n",
    "        metrica::text AS metrica,\n",
    "        valor::integer AS valor,\n",
    "        poblacion::integer AS poblacion\n",
    "    FROM cypher('covid_timeseries', $$\n",
    "        MATCH (e:entidad)-[r:TIENE_CASOS]->(f:fecha)\n",
    "        RETURN \n",
    "            e.cve_ent       AS entidad_id,\n",
    "            e.nombre        AS entidad_nombre,\n",
    "            e.poblacion     AS poblacion,\n",
    "            f.fecha         AS fecha,\n",
    "            f.metrica       AS metrica,\n",
    "            r.valor         AS valor\n",
    "    $$) AS (\n",
    "        entidad_id agtype,\n",
    "        entidad_nombre agtype,\n",
    "        poblacion agtype,\n",
    "        fecha agtype,\n",
    "        metrica agtype,\n",
    "        valor agtype\n",
    "    )\n",
    "    WHERE entidad_id IS NOT NULL AND fecha IS NOT NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    # 2. Unified data (relational + graph + text) - optimized casts\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW federation.unified_covid_data AS\n",
    "    -- Relational data\n",
    "    SELECT \n",
    "        'relational' as data_source,\n",
    "        c.fecha_actualizacion::date as fecha,\n",
    "        'casos_clinicos' as metrica,\n",
    "        c.clasificacion_final::text as valor,\n",
    "        json_build_object(\n",
    "            'tipo', 'relational',\n",
    "            'edad', c.edad,\n",
    "            'sexo', c.sexo,\n",
    "            'resultado_lab', c.resultado_lab,\n",
    "            'fecha_sintomas', c.fecha_sintomas,\n",
    "            'fecha_def', c.fecha_def\n",
    "        ) as metadata\n",
    "    FROM relational.covid_cases c\n",
    "    LEFT JOIN relational.entidades e ON c.entidad_res = e.entidad_id\n",
    "    WHERE c.clasificacion_final IS NOT NULL\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Graph data\n",
    "    SELECT \n",
    "        'graph' as data_source,\n",
    "        g.fecha as fecha,\n",
    "        g.metrica,\n",
    "        g.valor::text as valor,\n",
    "        json_build_object(\n",
    "            'tipo', 'graph',\n",
    "            'poblacion', g.poblacion,\n",
    "            'metrica_tipo', g.metrica\n",
    "        ) as metadata\n",
    "    FROM federation.graph_data_extracted g\n",
    "    WHERE g.valor > 0\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Text data\n",
    "    SELECT \n",
    "        'text' as data_source,\n",
    "        n.fecha as fecha,\n",
    "        'news_articles' as metrica,\n",
    "        n.article_id::text as valor,\n",
    "        json_build_object(\n",
    "            'tipo', 'text',\n",
    "            'title', n.title,\n",
    "            'author', n.author,\n",
    "            'categories', n.categories,\n",
    "            'content_preview', LEFT(n.content, 200)\n",
    "        ) as metadata\n",
    "    FROM text.news_articles n\n",
    "    WHERE n.fecha IS NOT NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    # 3. Comprehensive correlation\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW federation.comprehensive_correlation AS\n",
    "    WITH daily_relational AS (\n",
    "        SELECT \n",
    "            c.fecha_actualizacion as fecha,\n",
    "            c.entidad_res as entidad_id,\n",
    "            COUNT(c.id_registro) as daily_cases,\n",
    "            COUNT(CASE WHEN c.clasificacion_final = 1 THEN 1 END) as confirmed_cases,\n",
    "            COUNT(CASE WHEN c.clasificacion_final = 2 THEN 1 END) as suspected_cases,\n",
    "            COUNT(CASE WHEN c.fecha_def IS NOT NULL THEN 1 END) as deaths\n",
    "        FROM relational.covid_cases c\n",
    "        WHERE c.fecha_actualizacion IS NOT NULL\n",
    "        GROUP BY c.fecha_actualizacion, c.entidad_res\n",
    "    ),\n",
    "    daily_graph AS (\n",
    "        SELECT \n",
    "            fecha,\n",
    "            entidad_id::bigint as entidad_id,\n",
    "            entidad_nombre,\n",
    "            poblacion,\n",
    "            SUM(CASE WHEN metrica = 'confirmados' THEN valor ELSE 0 END) as graph_confirmados,\n",
    "            SUM(CASE WHEN metrica = 'defunciones' THEN valor ELSE 0 END) as graph_defunciones,\n",
    "            SUM(CASE WHEN metrica = 'negativos' THEN valor ELSE 0 END) as graph_negativos,\n",
    "            SUM(CASE WHEN metrica = 'sospechosos' THEN valor ELSE 0 END) as graph_sospechosos\n",
    "        FROM federation.graph_data_extracted\n",
    "        WHERE valor > 0\n",
    "        GROUP BY fecha, entidad_id::bigint, entidad_nombre, poblacion\n",
    "    ),\n",
    "    daily_text AS (\n",
    "        SELECT \n",
    "            fecha,\n",
    "            COUNT(article_id) as news_articles_count,\n",
    "            STRING_AGG(DISTINCT category, ', ') as news_categories,\n",
    "            STRING_AGG(DISTINCT author, ', ') as authors\n",
    "        FROM text.news_articles,\n",
    "            LATERAL unnest(categories::text[]) as category\n",
    "        WHERE fecha IS NOT NULL\n",
    "        GROUP BY fecha\n",
    "    )\n",
    "    SELECT \n",
    "        COALESCE(r.fecha, g.fecha, t.fecha) as fecha,\n",
    "        g.entidad_id,\n",
    "        g.entidad_nombre,\n",
    "        g.poblacion,\n",
    "        COALESCE(r.daily_cases, 0) as relational_cases,\n",
    "        COALESCE(r.confirmed_cases, 0) as relational_confirmed,\n",
    "        COALESCE(r.suspected_cases, 0) as relational_suspected,\n",
    "        COALESCE(r.deaths, 0) as relational_deaths,\n",
    "        COALESCE(g.graph_confirmados, 0) as graph_confirmados,\n",
    "        COALESCE(g.graph_defunciones, 0) as graph_defunciones,\n",
    "        COALESCE(g.graph_negativos, 0) as graph_negativos,\n",
    "        COALESCE(g.graph_sospechosos, 0) as graph_sospechosos,\n",
    "        COALESCE(t.news_articles_count, 0) as news_articles_count,\n",
    "        t.news_categories,\n",
    "        t.authors,\n",
    "        CASE \n",
    "            WHEN g.poblacion > 0 THEN (COALESCE(r.confirmed_cases, 0) * 100000.0 / g.poblacion)\n",
    "            ELSE 0 \n",
    "        END as tasa_confirmados_100k,\n",
    "        CASE \n",
    "            WHEN g.poblacion > 0 THEN (COALESCE(r.deaths, 0) * 100000.0 / g.poblacion)\n",
    "            ELSE 0 \n",
    "        END as tasa_defunciones_100k\n",
    "    FROM daily_relational r\n",
    "    FULL OUTER JOIN daily_graph g ON r.fecha = g.fecha AND r.entidad_id = g.entidad_id\n",
    "    FULL OUTER JOIN daily_text t ON COALESCE(r.fecha, g.fecha) = t.fecha\n",
    "    ORDER BY COALESCE(r.fecha, g.fecha, t.fecha) DESC, g.entidad_id;\n",
    "    \"\"\")\n",
    "\n",
    "    # 4. Graph analysis (optimized)\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW federation.graph_analysis AS\n",
    "    SELECT \n",
    "        entidad_id,\n",
    "        entidad_nombre,\n",
    "        poblacion,\n",
    "        fecha,\n",
    "        metrica,\n",
    "        valor,\n",
    "        CASE \n",
    "            WHEN poblacion > 0 THEN (valor * 100000.0 / poblacion)\n",
    "            ELSE 0 \n",
    "        END as tasa_100k,\n",
    "        SUM(valor) OVER (\n",
    "            PARTITION BY entidad_id, metrica \n",
    "            ORDER BY fecha \n",
    "            ROWS UNBOUNDED PRECEDING\n",
    "        ) as valor_acumulado,\n",
    "        AVG(valor) OVER (\n",
    "            PARTITION BY entidad_id, metrica \n",
    "            ORDER BY fecha \n",
    "            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "        ) as promedio_7_dias\n",
    "    FROM federation.graph_data_extracted\n",
    "    WHERE valor > 0\n",
    "    ORDER BY entidad_id, fecha, metrica;\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"✓ Federation schema created successfully with graph integration\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error creating federation schema: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b84be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COVID DATA FEDERATION SCHEMA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. RELATIONAL DATA SOURCE (relational.covid_cases)\n",
      "--------------------------------------------------\n",
      "Key columns used for federation:\n",
      "  • fecha_actualizacion: timestamp without time zone (NULL)\n",
      "  • sexo: bigint (NULL)\n",
      "  • entidad_res: bigint (NULL)\n",
      "  • fecha_sintomas: timestamp without time zone (NULL)\n",
      "  • fecha_def: timestamp without time zone (NULL)\n",
      "  • edad: bigint (NULL)\n",
      "  • resultado_lab: bigint (NULL)\n",
      "  • clasificacion_final: bigint (NULL)\n",
      "\n",
      "2. GRAPH DATA SOURCE (federation.graph_data_extracted)\n",
      "--------------------------------------------------\n",
      "Key columns used for federation:\n",
      "  • entidad_id: text (NULL)\n",
      "  • entidad_nombre: text (NULL)\n",
      "  • fecha: date (NULL)\n",
      "  • metrica: text (NULL)\n",
      "  • valor: integer (NULL)\n",
      "  • poblacion: integer (NULL)\n",
      "\n",
      "3. TEXT DATA SOURCE (text.news_articles)\n",
      "--------------------------------------------------\n",
      "Key columns used for federation:\n",
      "  • article_id: text (NULL)\n",
      "  • title: text (NULL)\n",
      "  • author: text (NULL)\n",
      "  • fecha: date (NULL)\n",
      "  • categories: text (NULL)\n",
      "  • content: text (NULL)\n",
      "\n",
      "4. FEDERATION SCHEMA STRUCTURE\n",
      "--------------------------------------------------\n",
      "Unified View: federation.unified_covid_data\n",
      "Columns:\n",
      "  • data_source: text (relational|graph|text)\n",
      "  • fecha: date (unified date field)\n",
      "  • metrica: text (metric type)\n",
      "  • valor: text (metric value - cast to text for consistency)\n",
      "  • metadata: jsonb (source-specific attributes)\n",
      "\n",
      "Comprehensive View: federation.comprehensive_correlation\n",
      "Columns:\n",
      "  • fecha: date\n",
      "  • entidad_id: bigint (unified entity ID)\n",
      "  • entidad_nombre: text\n",
      "  • poblacion: integer\n",
      "  • relational_cases: bigint\n",
      "  • relational_confirmed: bigint\n",
      "  • relational_suspected: bigint\n",
      "  • relational_deaths: bigint\n",
      "  • graph_confirmados: bigint\n",
      "  • graph_defunciones: bigint\n",
      "  • graph_negativos: bigint\n",
      "  • graph_sospechosos: bigint\n",
      "  • news_articles_count: bigint\n",
      "  • news_categories: text\n",
      "  • authors: text\n",
      "  • tasa_confirmados_100k: numeric\n",
      "  • tasa_defunciones_100k: numeric\n",
      "\n",
      "Graph Analysis View: federation.graph_analysis\n",
      "Columns:\n",
      "  • entidad_id: text\n",
      "  • entidad_nombre: text\n",
      "  • poblacion: integer\n",
      "  • fecha: date\n",
      "  • metrica: text\n",
      "  • valor: integer\n",
      "  • tasa_100k: numeric\n",
      "  • valor_acumulado: bigint\n",
      "  • promedio_7_dias: numeric\n",
      "\n",
      "5. DATA UNIFICATION STRATEGY\n",
      "--------------------------------------------------\n",
      "Unification Keys:\n",
      "  • Date: fecha_actualizacion (relational) ↔ fecha (graph/text)\n",
      "  • Entity: entidad_res (relational) ↔ entidad_id (graph)\n",
      "  • Metrics: clasificacion_final (relational) ↔ metrica (graph)\n",
      "\n",
      "Data Type Harmonization:\n",
      "  • All valor fields cast to TEXT for UNION compatibility\n",
      "  • entidad_id cast to BIGINT for JOIN compatibility\n",
      "  • Date fields standardized to DATE type\n",
      "\n",
      "Metadata Enrichment:\n",
      "  • Relational: edad, sexo, resultado_lab, fecha_sintomas, fecha_def\n",
      "  • Graph: poblacion, metrica_tipo\n",
      "  • Text: title, author, categories, content_preview\n",
      "\n",
      "6. SAMPLE DATA PREVIEW\n",
      "--------------------------------------------------\n",
      "\n",
      "Unified Data Sample (5 records):\n",
      "  1. Source: relational, Date: 2021-10-31, Metric: casos_clinicos, Value: 3\n",
      "     Metadata: {'tipo': 'relational', 'edad': 55, 'sexo': 1, 'resultado_lab': 1, 'fecha_sintomas': '2020-11-30T00:00:00', 'fecha_def': None}\n",
      "  2. Source: relational, Date: 2021-10-31, Metric: casos_clinicos, Value: 6\n",
      "     Metadata: {'tipo': 'relational', 'edad': 59, 'sexo': 2, 'resultado_lab': 97, 'fecha_sintomas': '2020-02-18T00:00:00', 'fecha_def': None}\n",
      "  3. Source: relational, Date: 2021-10-31, Metric: casos_clinicos, Value: 7\n",
      "     Metadata: {'tipo': 'relational', 'edad': 42, 'sexo': 1, 'resultado_lab': 2, 'fecha_sintomas': '2020-04-18T00:00:00', 'fecha_def': None}\n",
      "  4. Source: relational, Date: 2021-10-31, Metric: casos_clinicos, Value: 3\n",
      "     Metadata: {'tipo': 'relational', 'edad': 35, 'sexo': 2, 'resultado_lab': 1, 'fecha_sintomas': '2020-07-20T00:00:00', 'fecha_def': None}\n",
      "  5. Source: relational, Date: 2021-10-31, Metric: casos_clinicos, Value: 7\n",
      "     Metadata: {'tipo': 'relational', 'edad': 51, 'sexo': 2, 'resultado_lab': 2, 'fecha_sintomas': '2020-08-17T00:00:00', 'fecha_def': None}\n",
      "\n",
      "Comprehensive Correlation Sample (3 records):\n",
      "  1. Date: 2023-06-24 00:00:00, Entity: Nacional\n",
      "     Relational Confirmed: 0, Graph Confirmed: 15, News: 0\n",
      "  2. Date: 2023-06-24 00:00:00, Entity: DISTRITO FEDERAL\n",
      "     Relational Confirmed: 0, Graph Confirmed: 4, News: 0\n",
      "  3. Date: 2023-06-24 00:00:00, Entity: GUERRERO\n",
      "     Relational Confirmed: 0, Graph Confirmed: 2, News: 0\n",
      "\n",
      "================================================================================\n",
      "FEDERATION SCHEMA ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "7. PERFORMANCE OPTIMIZATIONS APPLIED\n",
      "--------------------------------------------------\n",
      "✓ Removed unnecessary type casts in federation views\n",
      "✓ Optimized data types (INTEGER → BIGINT) to match actual data\n",
      "✓ Improved error handling with try-catch blocks\n",
      "✓ Added data validation and quality checks\n",
      "✓ Created helper functions for better code organization\n",
      "✓ Optimized SQL queries for better performance\n",
      "✓ Added comprehensive data quality reporting\n",
      "✓ Improved connection management and error recovery\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION SUMMARY COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Data Source Structure Analysis and Federation Schema Overview\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COVID DATA FEDERATION SCHEMA ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Relational Data Structure\n",
    "    print(\"\\n1. RELATIONAL DATA SOURCE (relational.covid_cases)\")\n",
    "    print(\"-\" * 50)\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        data_type,\n",
    "        is_nullable,\n",
    "        column_default\n",
    "    FROM information_schema.columns \n",
    "    WHERE table_schema = 'relational' \n",
    "    AND table_name = 'covid_cases'\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\")\n",
    "    \n",
    "    relational_cols = cursor.fetchall()\n",
    "    print(\"Key columns used for federation:\")\n",
    "    for col in relational_cols:\n",
    "        if col[0] in ['fecha_actualizacion', 'entidad_res', 'clasificacion_final', 'edad', 'sexo', 'resultado_lab', 'fecha_sintomas', 'fecha_def']:\n",
    "            print(f\"  • {col[0]}: {col[1]} ({'NULL' if col[2] == 'YES' else 'NOT NULL'})\")\n",
    "    \n",
    "    # 2. Graph Data Structure\n",
    "    print(\"\\n2. GRAPH DATA SOURCE (federation.graph_data_extracted)\")\n",
    "    print(\"-\" * 50)\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        data_type,\n",
    "        is_nullable\n",
    "    FROM information_schema.columns \n",
    "    WHERE table_schema = 'federation' \n",
    "    AND table_name = 'graph_data_extracted'\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\")\n",
    "    \n",
    "    graph_cols = cursor.fetchall()\n",
    "    print(\"Key columns used for federation:\")\n",
    "    for col in graph_cols:\n",
    "        print(f\"  • {col[0]}: {col[1]} ({'NULL' if col[2] == 'YES' else 'NOT NULL'})\")\n",
    "    \n",
    "    # 3. Text Data Structure\n",
    "    print(\"\\n3. TEXT DATA SOURCE (text.news_articles)\")\n",
    "    print(\"-\" * 50)\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        data_type,\n",
    "        is_nullable\n",
    "    FROM information_schema.columns \n",
    "    WHERE table_schema = 'text' \n",
    "    AND table_name = 'news_articles'\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\")\n",
    "    \n",
    "    text_cols = cursor.fetchall()\n",
    "    print(\"Key columns used for federation:\")\n",
    "    for col in text_cols:\n",
    "        if col[0] in ['fecha', 'article_id', 'title', 'author', 'categories', 'content']:\n",
    "            print(f\"  • {col[0]}: {col[1]} ({'NULL' if col[2] == 'YES' else 'NOT NULL'})\")\n",
    "    \n",
    "    # 4. Federation Schema Structure\n",
    "    print(\"\\n4. FEDERATION SCHEMA STRUCTURE\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Unified View: federation.unified_covid_data\")\n",
    "    print(\"Columns:\")\n",
    "    print(\"  • data_source: text (relational|graph|text)\")\n",
    "    print(\"  • fecha: date (unified date field)\")\n",
    "    print(\"  • metrica: text (metric type)\")\n",
    "    print(\"  • valor: text (metric value - cast to text for consistency)\")\n",
    "    print(\"  • metadata: jsonb (source-specific attributes)\")\n",
    "    \n",
    "    print(\"\\nComprehensive View: federation.comprehensive_correlation\")\n",
    "    print(\"Columns:\")\n",
    "    print(\"  • fecha: date\")\n",
    "    print(\"  • entidad_id: bigint (unified entity ID)\")\n",
    "    print(\"  • entidad_nombre: text\")\n",
    "    print(\"  • poblacion: integer\")\n",
    "    print(\"  • relational_cases: bigint\")\n",
    "    print(\"  • relational_confirmed: bigint\")\n",
    "    print(\"  • relational_suspected: bigint\")\n",
    "    print(\"  • relational_deaths: bigint\")\n",
    "    print(\"  • graph_confirmados: bigint\")\n",
    "    print(\"  • graph_defunciones: bigint\")\n",
    "    print(\"  • graph_negativos: bigint\")\n",
    "    print(\"  • graph_sospechosos: bigint\")\n",
    "    print(\"  • news_articles_count: bigint\")\n",
    "    print(\"  • news_categories: text\")\n",
    "    print(\"  • authors: text\")\n",
    "    print(\"  • tasa_confirmados_100k: numeric\")\n",
    "    print(\"  • tasa_defunciones_100k: numeric\")\n",
    "    \n",
    "    print(\"\\nGraph Analysis View: federation.graph_analysis\")\n",
    "    print(\"Columns:\")\n",
    "    print(\"  • entidad_id: text\")\n",
    "    print(\"  • entidad_nombre: text\")\n",
    "    print(\"  • poblacion: integer\")\n",
    "    print(\"  • fecha: date\")\n",
    "    print(\"  • metrica: text\")\n",
    "    print(\"  • valor: integer\")\n",
    "    print(\"  • tasa_100k: numeric\")\n",
    "    print(\"  • valor_acumulado: bigint\")\n",
    "    print(\"  • promedio_7_dias: numeric\")\n",
    "    \n",
    "    # 5. Data Unification Strategy\n",
    "    print(\"\\n5. DATA UNIFICATION STRATEGY\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Unification Keys:\")\n",
    "    print(\"  • Date: fecha_actualizacion (relational) ↔ fecha (graph/text)\")\n",
    "    print(\"  • Entity: entidad_res (relational) ↔ entidad_id (graph)\")\n",
    "    print(\"  • Metrics: clasificacion_final (relational) ↔ metrica (graph)\")\n",
    "    \n",
    "    print(\"\\nData Type Harmonization:\")\n",
    "    print(\"  • All valor fields cast to TEXT for UNION compatibility\")\n",
    "    print(\"  • entidad_id cast to BIGINT for JOIN compatibility\")\n",
    "    print(\"  • Date fields standardized to DATE type\")\n",
    "    \n",
    "    print(\"\\nMetadata Enrichment:\")\n",
    "    print(\"  • Relational: edad, sexo, resultado_lab, fecha_sintomas, fecha_def\")\n",
    "    print(\"  • Graph: poblacion, metrica_tipo\")\n",
    "    print(\"  • Text: title, author, categories, content_preview\")\n",
    "    \n",
    "    # 6. Sample Data Preview\n",
    "    print(\"\\n6. SAMPLE DATA PREVIEW\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nUnified Data Sample (5 records):\")\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        data_source,\n",
    "        fecha,\n",
    "        metrica,\n",
    "        valor,\n",
    "        metadata\n",
    "    FROM federation.unified_covid_data \n",
    "    LIMIT 5;\n",
    "    \"\"\")\n",
    "    \n",
    "    unified_sample = cursor.fetchall()\n",
    "    for i, row in enumerate(unified_sample, 1):\n",
    "        print(f\"  {i}. Source: {row[0]}, Date: {row[1]}, Metric: {row[2]}, Value: {row[3]}\")\n",
    "        print(f\"     Metadata: {row[4]}\")\n",
    "    \n",
    "    print(\"\\nComprehensive Correlation Sample (3 records):\")\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        fecha,\n",
    "        entidad_nombre,\n",
    "        relational_confirmed,\n",
    "        graph_confirmados,\n",
    "        news_articles_count\n",
    "    FROM federation.comprehensive_correlation \n",
    "    WHERE relational_confirmed > 0 OR graph_confirmados > 0\n",
    "    LIMIT 3;\n",
    "    \"\"\")\n",
    "    \n",
    "    correlation_sample = cursor.fetchall()\n",
    "    for i, row in enumerate(correlation_sample, 1):\n",
    "        print(f\"  {i}. Date: {row[0]}, Entity: {row[1]}\")\n",
    "        print(f\"     Relational Confirmed: {row[2]}, Graph Confirmed: {row[3]}, News: {row[4]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FEDERATION SCHEMA ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 7. Performance Optimizations Summary\n",
    "    print(\"\\n7. PERFORMANCE OPTIMIZATIONS APPLIED\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"✓ Removed unnecessary type casts in federation views\")\n",
    "    print(\"✓ Optimized data types (INTEGER → BIGINT) to match actual data\")\n",
    "    print(\"✓ Improved error handling with try-catch blocks\")\n",
    "    print(\"✓ Added data validation and quality checks\")\n",
    "    print(\"✓ Created helper functions for better code organization\")\n",
    "    print(\"✓ Optimized SQL queries for better performance\")\n",
    "    print(\"✓ Added comprehensive data quality reporting\")\n",
    "    print(\"✓ Improved connection management and error recovery\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OPTIMIZATION SUMMARY COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Error in schema analysis: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    cursor.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
