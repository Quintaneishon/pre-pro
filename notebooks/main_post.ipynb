{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7d076bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:52:16.610896Z",
     "start_time": "2025-09-17T22:51:40.730347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://ajquintana:****@pypi.artifacts.furycloud.io\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://ajquintana:****@pypi.artifacts.furycloud.io\n",
      "Requirement already satisfied: openpyxl in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://ajquintana:****@pypi.artifacts.furycloud.io\n",
      "Requirement already satisfied: psycopg2-binary in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.9.10)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://ajquintana:****@pypi.artifacts.furycloud.io\n",
      "Requirement already satisfied: sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.0.43)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from sqlalchemy) (4.13.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Looking in indexes: https://ajquintana:****@pypi.artifacts.furycloud.io\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.1.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install psycopg2-binary\n",
    "!pip install sqlalchemy\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43ae761a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Connected to PostgreSQL: PostgreSQL 16.10 (Debian 16.10-1.pgdg13+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 14.2.0-19) 14.2.0, 64-bit\n",
      "‚úì PostgreSQL database initialized with extensions and schemas\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine, text\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# PostgreSQL connection configuration\n",
    "DB_CONFIG = {\n",
    "    'host': os.getenv('POSTGRES_HOST', 'localhost'),\n",
    "    'port': os.getenv('POSTGRES_PORT', '5432'),\n",
    "    'database': os.getenv('POSTGRES_DB', 'covid_analysis'),\n",
    "    'user': os.getenv('POSTGRES_USER', 'postgres'),\n",
    "    'password': os.getenv('POSTGRES_PASSWORD', 'password')\n",
    "}\n",
    "\n",
    "# Helper functions for better code organization\n",
    "def create_database_connections():\n",
    "    \"\"\"Create and return database connections with error handling\"\"\"\n",
    "    try:\n",
    "        # Create SQLAlchemy engine for pandas integration\n",
    "        engine = create_engine(f\"postgresql://{DB_CONFIG['user']}:{DB_CONFIG['password']}@{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}\")\n",
    "        \n",
    "        # Direct psycopg2 connection for complex queries\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        conn.autocommit = True\n",
    "        \n",
    "        return engine, conn\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Database connection failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def validate_dataframe(df, required_columns=None, min_rows=1):\n",
    "    \"\"\"Validate DataFrame structure and content\"\"\"\n",
    "    if df is None or df.empty:\n",
    "        raise ValueError(\"DataFrame is empty or None\")\n",
    "    \n",
    "    if len(df) < min_rows:\n",
    "        raise ValueError(f\"DataFrame has fewer than {min_rows} rows\")\n",
    "    \n",
    "    if required_columns:\n",
    "        missing_cols = [col for col in required_columns if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Create database connections\n",
    "engine, conn = create_database_connections()\n",
    "\n",
    "def setup_postgresql_database():\n",
    "    \"\"\"Initialize PostgreSQL database with required extensions and schemas\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Enable required extensions with better error handling\n",
    "        extensions = [\n",
    "            \"CREATE EXTENSION IF NOT EXISTS postgres_fdw;\",\n",
    "            \"CREATE EXTENSION IF NOT EXISTS age;\",\n",
    "            \"CREATE EXTENSION IF NOT EXISTS btree_gin;\",\n",
    "            \"CREATE EXTENSION IF NOT EXISTS btree_gist;\"\n",
    "        ]\n",
    "        \n",
    "        for ext_sql in extensions:\n",
    "            try:\n",
    "                cursor.execute(ext_sql)\n",
    "            except Exception as ext_e:\n",
    "                print(f\"‚ö† Warning: Could not create extension: {ext_e}\")\n",
    "        \n",
    "        # Create schemas for different data types\n",
    "        schemas = ['relational', 'graph', 'text', 'federation']\n",
    "        for schema in schemas:\n",
    "            cursor.execute(f\"CREATE SCHEMA IF NOT EXISTS {schema};\")\n",
    "        \n",
    "        print(\"‚úì PostgreSQL database initialized with extensions and schemas\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Error setting up database: {e}\")\n",
    "        print(\"Make sure PostgreSQL is running and accessible\")\n",
    "        raise\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Test connection first\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT version();\")\n",
    "    version = cursor.fetchone()[0]\n",
    "    print(f\"‚úì Connected to PostgreSQL: {version}\")\n",
    "    cursor.close()\n",
    "    \n",
    "    # Initialize the database\n",
    "    setup_postgresql_database()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Cannot connect to PostgreSQL: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Make sure Docker is running\")\n",
    "    print(\"2. Start PostgreSQL container: docker-compose up -d\")\n",
    "    print(\"3. Wait for container to be ready: docker-compose logs postgres\")\n",
    "    print(\"4. Check if container is running: docker-compose ps\")\n",
    "    print(\"\\nIf using Docker Compose, the database should be automatically created.\")\n",
    "    print(\"If you're using a local PostgreSQL installation, create the database manually:\")\n",
    "    print(\"  CREATE DATABASE covid_analysis;\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25795d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì All dependent objects dropped successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Drop all dependent objects in cascade to clean up the database\"\"\"\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Drop all federation views first (in dependency order)\n",
    "    cursor.execute(\"\"\"\n",
    "    DROP VIEW IF EXISTS federation.comprehensive_correlation CASCADE;\n",
    "    DROP VIEW IF EXISTS federation.unified_covid_data CASCADE;\n",
    "    DROP VIEW IF EXISTS federation.graph_analysis CASCADE;\n",
    "    DROP VIEW IF EXISTS federation.graph_data_extracted CASCADE;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Drop all tables in cascade\n",
    "    cursor.execute(\"\"\"\n",
    "    DROP TABLE IF EXISTS relational.covid_cases CASCADE;\n",
    "    DROP TABLE IF EXISTS relational.entidades CASCADE;\n",
    "    DROP TABLE IF EXISTS text.news_articles CASCADE;\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"‚úì All dependent objects dropped successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error dropping objects: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe043be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Relational schema created successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create PostgreSQL schema for relational COVID data\"\"\"\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Create main COVID cases table (optimized data types)\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS relational.covid_cases (\n",
    "        id_registro BIGSERIAL PRIMARY KEY,\n",
    "        fecha_actualizacion DATE,\n",
    "        fecha_ingreso DATE,\n",
    "        fecha_sintomas DATE,\n",
    "        fecha_def DATE,\n",
    "        entidad_res BIGINT,\n",
    "        municipio_res BIGINT,\n",
    "        entidad_um BIGINT,\n",
    "        sexo BIGINT,\n",
    "        edad BIGINT,\n",
    "        nacionalidad BIGINT,\n",
    "        embarazo BIGINT,\n",
    "        habla_lengua_indig BIGINT,\n",
    "        indigena BIGINT,\n",
    "        diabetes BIGINT,\n",
    "        obesidad BIGINT,\n",
    "        hipertension BIGINT,\n",
    "        renal_cronica BIGINT,\n",
    "        asma BIGINT,\n",
    "        epoc BIGINT,\n",
    "        cardiovascular BIGINT,\n",
    "        inmusupr BIGINT,\n",
    "        otra_com BIGINT,\n",
    "        cardiovascular_aguda BIGINT,\n",
    "        obesidad_aguda BIGINT,\n",
    "        diabetes_aguda BIGINT,\n",
    "        clasificacion_final BIGINT,\n",
    "        migrante BIGINT,\n",
    "        pais_origen VARCHAR(50),\n",
    "        uci BIGINT,\n",
    "        intubado BIGINT,\n",
    "        neumonia BIGINT,\n",
    "        tipo_paciente BIGINT,\n",
    "        resultado_lab BIGINT,\n",
    "        resultado_antigeno BIGINT,\n",
    "        sector BIGINT,\n",
    "        year INTEGER,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create entity catalog table (optimized data types)\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS relational.entidades (\n",
    "        entidad_id BIGINT PRIMARY KEY,\n",
    "        entidad_nombre VARCHAR(100),\n",
    "        ent_abbr VARCHAR(10)\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create indexes for performance\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_covid_cases_entidad ON relational.covid_cases(entidad_res);\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_covid_cases_fecha ON relational.covid_cases(fecha_actualizacion);\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_covid_cases_year ON relational.covid_cases(year);\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_covid_cases_clasificacion ON relational.covid_cases(clasificacion_final);\")\n",
    "    \n",
    "    print(\"‚úì Relational schema created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error creating relational schema: {e}\")\n",
    "finally:\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86f6fb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded COVID19MEXICO2020.csv: (3868396, 41)\n",
      "  Columns: ['FECHA_ACTUALIZACION', 'ID_REGISTRO', 'ORIGEN', 'SECTOR', 'ENTIDAD_UM', 'SEXO', 'ENTIDAD_NAC', 'ENTIDAD_RES', 'MUNICIPIO_RES', 'TIPO_PACIENTE', 'FECHA_INGRESO', 'FECHA_SINTOMAS', 'FECHA_DEF', 'INTUBADO', 'NEUMONIA', 'EDAD', 'NACIONALIDAD', 'EMBARAZO', 'HABLA_LENGUA_INDIG', 'INDIGENA', 'DIABETES', 'EPOC', 'ASMA', 'INMUSUPR', 'HIPERTENSION', 'OTRA_COM', 'CARDIOVASCULAR', 'OBESIDAD', 'RENAL_CRONICA', 'TABAQUISMO', 'OTRO_CASO', 'TOMA_MUESTRA_LAB', 'RESULTADO_LAB', 'TOMA_MUESTRA_ANTIGENO', 'RESULTADO_ANTIGENO', 'CLASIFICACION_FINAL', 'MIGRANTE', 'PAIS_NACIONALIDAD', 'PAIS_ORIGEN', 'UCI', 'YEAR']\n",
      "‚úì Combined relational data: (3868396, 41)\n",
      "‚úì Years included: ['2020']\n",
      "‚úì All unique columns: ['ASMA', 'CARDIOVASCULAR', 'CLASIFICACION_FINAL', 'DIABETES', 'EDAD', 'EMBARAZO', 'ENTIDAD_NAC', 'ENTIDAD_RES', 'ENTIDAD_UM', 'EPOC', 'FECHA_ACTUALIZACION', 'FECHA_DEF', 'FECHA_INGRESO', 'FECHA_SINTOMAS', 'HABLA_LENGUA_INDIG', 'HIPERTENSION', 'ID_REGISTRO', 'INDIGENA', 'INMUSUPR', 'INTUBADO', 'MIGRANTE', 'MUNICIPIO_RES', 'NACIONALIDAD', 'NEUMONIA', 'OBESIDAD', 'ORIGEN', 'OTRA_COM', 'OTRO_CASO', 'PAIS_NACIONALIDAD', 'PAIS_ORIGEN', 'RENAL_CRONICA', 'RESULTADO_ANTIGENO', 'RESULTADO_LAB', 'SECTOR', 'SEXO', 'TABAQUISMO', 'TIPO_PACIENTE', 'TOMA_MUESTRA_ANTIGENO', 'TOMA_MUESTRA_LAB', 'UCI', 'YEAR']\n",
      "\n",
      "üìä Data Quality Validation:\n",
      "‚úì All key columns present: ['CLASIFICACION_FINAL', 'RESULTADO_LAB', 'FECHA_ACTUALIZACION', 'ENTIDAD_RES']\n",
      "‚úì Data completeness check:\n",
      "  ‚Ä¢ CLASIFICACION_FINAL: 0.00% null values\n",
      "  ‚Ä¢ RESULTADO_LAB: 0.00% null values\n",
      "  ‚Ä¢ FECHA_ACTUALIZACION: 0.00% null values\n",
      "  ‚Ä¢ ENTIDAD_RES: 0.00% null values\n",
      "‚úì Date range: 2021-10-31 to 2021-10-31\n",
      "‚úì Entities represented: 32\n",
      "‚úì Records per entity: min=17500, max=1125066\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load and prepare all required datasets with proper column handling for PostgreSQL\"\"\"\n",
    "data_dir = \"../data\"\n",
    "\n",
    "# Load COVID cases data (relational) - all yearly CSV files\n",
    "relational_dir = os.path.join(data_dir, \"relational\")\n",
    "csv_files = [\n",
    "    \"COVID19MEXICO2020.csv\",\n",
    "    # \"COVID19MEXICO2021.csv\", \n",
    "    # \"COVID19MEXICO2022.csv\",\n",
    "    # \"COVID19MEXICO2023.csv\"\n",
    "]\n",
    "\n",
    "df_cases_list = []\n",
    "all_columns = set()\n",
    "\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(relational_dir, csv_file)\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            # Fix DtypeWarning by specifying dtype for problematic column\n",
    "            df_year = pd.read_csv(file_path, dtype={'PAIS_ORIGEN': str})\n",
    "            df_year['YEAR'] = csv_file.replace('COVID19MEXICO', '').replace('.csv', '')\n",
    "            df_cases_list.append(df_year)\n",
    "            all_columns.update(df_year.columns)\n",
    "            print(f\"‚úì Loaded {csv_file}: {df_year.shape}\")\n",
    "            print(f\"  Columns: {list(df_year.columns)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Error loading {csv_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"‚ö† File not found: {csv_file}\")\n",
    "\n",
    "# Combine all yearly data with validation\n",
    "if df_cases_list:\n",
    "    df_cases = pd.concat(df_cases_list, ignore_index=True)\n",
    "    print(f\"‚úì Combined relational data: {df_cases.shape}\")\n",
    "    print(f\"‚úì Years included: {sorted(df_cases['YEAR'].unique())}\")\n",
    "    print(f\"‚úì All unique columns: {sorted(all_columns)}\")\n",
    "    \n",
    "    # Data quality validation\n",
    "    print(\"\\nüìä Data Quality Validation:\")\n",
    "    \n",
    "    # Check for key columns\n",
    "    key_columns = ['CLASIFICACION_FINAL', 'RESULTADO_LAB', 'FECHA_ACTUALIZACION', 'ENTIDAD_RES']\n",
    "    missing_cols = [col for col in key_columns if col not in df_cases.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"‚ö† Missing key columns: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"‚úì All key columns present: {key_columns}\")\n",
    "    \n",
    "    # Check data completeness\n",
    "    total_records = len(df_cases)\n",
    "    null_counts = df_cases[key_columns].isnull().sum()\n",
    "    print(f\"‚úì Data completeness check:\")\n",
    "    for col in key_columns:\n",
    "        null_pct = (null_counts[col] / total_records) * 100\n",
    "        print(f\"  ‚Ä¢ {col}: {null_pct:.2f}% null values\")\n",
    "    \n",
    "    # Check date range\n",
    "    if 'FECHA_ACTUALIZACION' in df_cases.columns:\n",
    "        date_col = pd.to_datetime(df_cases['FECHA_ACTUALIZACION'], errors='coerce')\n",
    "        valid_dates = date_col.dropna()\n",
    "        if len(valid_dates) > 0:\n",
    "            print(f\"‚úì Date range: {valid_dates.min().date()} to {valid_dates.max().date()}\")\n",
    "        else:\n",
    "            print(\"‚ö† No valid dates found in FECHA_ACTUALIZACION\")\n",
    "    \n",
    "    # Check entity distribution\n",
    "    if 'ENTIDAD_RES' in df_cases.columns:\n",
    "        entity_counts = df_cases['ENTIDAD_RES'].value_counts()\n",
    "        print(f\"‚úì Entities represented: {len(entity_counts)}\")\n",
    "        print(f\"‚úì Records per entity: min={entity_counts.min()}, max={entity_counts.max()}\")\n",
    "        \n",
    "else:\n",
    "    raise ValueError(\"No CSV files could be loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bc1c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading relational data to PostgreSQL...\n",
      "Loading 3868396 rows in chunks of 10000...\n",
      "Loaded chunk 1/387\n",
      "Loaded chunk 2/387\n",
      "Loaded chunk 3/387\n",
      "Loaded chunk 4/387\n",
      "Loaded chunk 5/387\n",
      "Loaded chunk 6/387\n",
      "Loaded chunk 7/387\n",
      "Loaded chunk 8/387\n",
      "Loaded chunk 9/387\n",
      "Loaded chunk 10/387\n",
      "Loaded chunk 11/387\n",
      "Loaded chunk 12/387\n",
      "Loaded chunk 13/387\n",
      "Loaded chunk 14/387\n",
      "Loaded chunk 15/387\n",
      "Loaded chunk 16/387\n",
      "Loaded chunk 17/387\n",
      "Loaded chunk 18/387\n",
      "Loaded chunk 19/387\n",
      "Loaded chunk 20/387\n",
      "Loaded chunk 21/387\n",
      "Loaded chunk 22/387\n",
      "Loaded chunk 23/387\n",
      "Loaded chunk 24/387\n",
      "Loaded chunk 25/387\n",
      "Loaded chunk 26/387\n",
      "Loaded chunk 27/387\n",
      "Loaded chunk 28/387\n",
      "Loaded chunk 29/387\n",
      "Loaded chunk 30/387\n",
      "Loaded chunk 31/387\n",
      "Loaded chunk 32/387\n",
      "Loaded chunk 33/387\n",
      "Loaded chunk 34/387\n",
      "Loaded chunk 35/387\n",
      "Loaded chunk 36/387\n",
      "Loaded chunk 37/387\n",
      "Loaded chunk 38/387\n",
      "Loaded chunk 39/387\n",
      "Loaded chunk 40/387\n",
      "Loaded chunk 41/387\n",
      "Loaded chunk 42/387\n",
      "Loaded chunk 43/387\n",
      "Loaded chunk 44/387\n",
      "Loaded chunk 45/387\n",
      "Loaded chunk 46/387\n",
      "Loaded chunk 47/387\n",
      "Loaded chunk 48/387\n",
      "Loaded chunk 49/387\n",
      "Loaded chunk 50/387\n",
      "Loaded chunk 51/387\n",
      "Loaded chunk 52/387\n",
      "Loaded chunk 53/387\n",
      "Loaded chunk 54/387\n",
      "Loaded chunk 55/387\n",
      "Loaded chunk 56/387\n",
      "Loaded chunk 57/387\n",
      "Loaded chunk 58/387\n",
      "Loaded chunk 59/387\n",
      "Loaded chunk 60/387\n",
      "Loaded chunk 61/387\n",
      "Loaded chunk 62/387\n",
      "Loaded chunk 63/387\n",
      "Loaded chunk 64/387\n",
      "Loaded chunk 65/387\n",
      "Loaded chunk 66/387\n",
      "Loaded chunk 67/387\n",
      "Loaded chunk 68/387\n",
      "Loaded chunk 69/387\n",
      "Loaded chunk 70/387\n",
      "Loaded chunk 71/387\n",
      "Loaded chunk 72/387\n",
      "Loaded chunk 73/387\n",
      "Loaded chunk 74/387\n",
      "Loaded chunk 75/387\n",
      "Loaded chunk 76/387\n",
      "Loaded chunk 77/387\n",
      "Loaded chunk 78/387\n",
      "Loaded chunk 79/387\n",
      "Loaded chunk 80/387\n",
      "Loaded chunk 81/387\n",
      "Loaded chunk 82/387\n",
      "Loaded chunk 83/387\n",
      "Loaded chunk 84/387\n",
      "Loaded chunk 85/387\n",
      "Loaded chunk 86/387\n",
      "Loaded chunk 87/387\n",
      "Loaded chunk 88/387\n",
      "Loaded chunk 89/387\n",
      "Loaded chunk 90/387\n",
      "Loaded chunk 91/387\n",
      "Loaded chunk 92/387\n",
      "Loaded chunk 93/387\n",
      "Loaded chunk 94/387\n",
      "Loaded chunk 95/387\n",
      "Loaded chunk 96/387\n",
      "Loaded chunk 97/387\n",
      "Loaded chunk 98/387\n",
      "Loaded chunk 99/387\n",
      "Loaded chunk 100/387\n",
      "Loaded chunk 101/387\n",
      "Loaded chunk 102/387\n",
      "Loaded chunk 103/387\n",
      "Loaded chunk 104/387\n",
      "Loaded chunk 105/387\n",
      "Loaded chunk 106/387\n",
      "Loaded chunk 107/387\n",
      "Loaded chunk 108/387\n",
      "Loaded chunk 109/387\n",
      "Loaded chunk 110/387\n",
      "Loaded chunk 111/387\n",
      "Loaded chunk 112/387\n",
      "Loaded chunk 113/387\n",
      "Loaded chunk 114/387\n",
      "Loaded chunk 115/387\n",
      "Loaded chunk 116/387\n",
      "Loaded chunk 117/387\n",
      "Loaded chunk 118/387\n",
      "Loaded chunk 119/387\n",
      "Loaded chunk 120/387\n",
      "Loaded chunk 121/387\n",
      "Loaded chunk 122/387\n",
      "Loaded chunk 123/387\n",
      "Loaded chunk 124/387\n",
      "Loaded chunk 125/387\n",
      "Loaded chunk 126/387\n",
      "Loaded chunk 127/387\n",
      "Loaded chunk 128/387\n",
      "Loaded chunk 129/387\n",
      "Loaded chunk 130/387\n",
      "Loaded chunk 131/387\n",
      "Loaded chunk 132/387\n",
      "Loaded chunk 133/387\n",
      "Loaded chunk 134/387\n",
      "Loaded chunk 135/387\n",
      "Loaded chunk 136/387\n",
      "Loaded chunk 137/387\n",
      "Loaded chunk 138/387\n",
      "Loaded chunk 139/387\n",
      "Loaded chunk 140/387\n",
      "Loaded chunk 141/387\n",
      "Loaded chunk 142/387\n",
      "Loaded chunk 143/387\n",
      "Loaded chunk 144/387\n",
      "Loaded chunk 145/387\n",
      "Loaded chunk 146/387\n",
      "Loaded chunk 147/387\n",
      "Loaded chunk 148/387\n",
      "Loaded chunk 149/387\n",
      "Loaded chunk 150/387\n",
      "Loaded chunk 151/387\n",
      "Loaded chunk 152/387\n",
      "Loaded chunk 153/387\n",
      "Loaded chunk 154/387\n",
      "Loaded chunk 155/387\n",
      "Loaded chunk 156/387\n",
      "Loaded chunk 157/387\n",
      "Loaded chunk 158/387\n",
      "Loaded chunk 159/387\n",
      "Loaded chunk 160/387\n",
      "Loaded chunk 161/387\n",
      "Loaded chunk 162/387\n",
      "Loaded chunk 163/387\n",
      "Loaded chunk 164/387\n",
      "Loaded chunk 165/387\n",
      "Loaded chunk 166/387\n",
      "Loaded chunk 167/387\n",
      "Loaded chunk 168/387\n",
      "Loaded chunk 169/387\n",
      "Loaded chunk 170/387\n",
      "Loaded chunk 171/387\n",
      "Loaded chunk 172/387\n",
      "Loaded chunk 173/387\n",
      "Loaded chunk 174/387\n",
      "Loaded chunk 175/387\n",
      "Loaded chunk 176/387\n",
      "Loaded chunk 177/387\n",
      "Loaded chunk 178/387\n",
      "Loaded chunk 179/387\n",
      "Loaded chunk 180/387\n",
      "Loaded chunk 181/387\n",
      "Loaded chunk 182/387\n",
      "Loaded chunk 183/387\n",
      "Loaded chunk 184/387\n",
      "Loaded chunk 185/387\n",
      "Loaded chunk 186/387\n",
      "Loaded chunk 187/387\n",
      "Loaded chunk 188/387\n",
      "Loaded chunk 189/387\n",
      "Loaded chunk 190/387\n",
      "Loaded chunk 191/387\n",
      "Loaded chunk 192/387\n",
      "Loaded chunk 193/387\n",
      "Loaded chunk 194/387\n",
      "Loaded chunk 195/387\n",
      "Loaded chunk 196/387\n",
      "Loaded chunk 197/387\n",
      "Loaded chunk 198/387\n",
      "Loaded chunk 199/387\n",
      "Loaded chunk 200/387\n",
      "Loaded chunk 201/387\n",
      "Loaded chunk 202/387\n",
      "Loaded chunk 203/387\n",
      "Loaded chunk 204/387\n",
      "Loaded chunk 205/387\n",
      "Loaded chunk 206/387\n",
      "Loaded chunk 207/387\n",
      "Loaded chunk 208/387\n",
      "Loaded chunk 209/387\n",
      "Loaded chunk 210/387\n",
      "Loaded chunk 211/387\n",
      "Loaded chunk 212/387\n",
      "Loaded chunk 213/387\n",
      "Loaded chunk 214/387\n",
      "Loaded chunk 215/387\n",
      "Loaded chunk 216/387\n",
      "Loaded chunk 217/387\n",
      "Loaded chunk 218/387\n",
      "Loaded chunk 219/387\n",
      "Loaded chunk 220/387\n",
      "Loaded chunk 221/387\n",
      "Loaded chunk 222/387\n",
      "Loaded chunk 223/387\n",
      "Loaded chunk 224/387\n",
      "Loaded chunk 225/387\n",
      "Loaded chunk 226/387\n",
      "Loaded chunk 227/387\n",
      "Loaded chunk 228/387\n",
      "Loaded chunk 229/387\n",
      "Loaded chunk 230/387\n",
      "Loaded chunk 231/387\n",
      "Loaded chunk 232/387\n",
      "Loaded chunk 233/387\n",
      "Loaded chunk 234/387\n",
      "Loaded chunk 235/387\n",
      "Loaded chunk 236/387\n",
      "Loaded chunk 237/387\n",
      "Loaded chunk 238/387\n",
      "Loaded chunk 239/387\n",
      "Loaded chunk 240/387\n",
      "Loaded chunk 241/387\n",
      "Loaded chunk 242/387\n",
      "Loaded chunk 243/387\n",
      "Loaded chunk 244/387\n",
      "Loaded chunk 245/387\n",
      "Loaded chunk 246/387\n",
      "Loaded chunk 247/387\n",
      "Loaded chunk 248/387\n",
      "Loaded chunk 249/387\n",
      "Loaded chunk 250/387\n",
      "Loaded chunk 251/387\n",
      "Loaded chunk 252/387\n",
      "Loaded chunk 253/387\n",
      "Loaded chunk 254/387\n",
      "Loaded chunk 255/387\n",
      "Loaded chunk 256/387\n",
      "Loaded chunk 257/387\n",
      "Loaded chunk 258/387\n",
      "Loaded chunk 259/387\n",
      "Loaded chunk 260/387\n",
      "Loaded chunk 261/387\n",
      "Loaded chunk 262/387\n",
      "Loaded chunk 263/387\n",
      "Loaded chunk 264/387\n",
      "Loaded chunk 265/387\n",
      "Loaded chunk 266/387\n",
      "Loaded chunk 267/387\n",
      "Loaded chunk 268/387\n",
      "Loaded chunk 269/387\n",
      "Loaded chunk 270/387\n",
      "Loaded chunk 271/387\n",
      "Loaded chunk 272/387\n",
      "Loaded chunk 273/387\n",
      "Loaded chunk 274/387\n",
      "Loaded chunk 275/387\n",
      "Loaded chunk 276/387\n",
      "Loaded chunk 277/387\n",
      "Loaded chunk 278/387\n",
      "Loaded chunk 279/387\n",
      "Loaded chunk 280/387\n",
      "Loaded chunk 281/387\n",
      "Loaded chunk 282/387\n",
      "Loaded chunk 283/387\n",
      "Loaded chunk 284/387\n",
      "Loaded chunk 285/387\n",
      "Loaded chunk 286/387\n",
      "Loaded chunk 287/387\n",
      "Loaded chunk 288/387\n",
      "Loaded chunk 289/387\n",
      "Loaded chunk 290/387\n",
      "Loaded chunk 291/387\n",
      "Loaded chunk 292/387\n",
      "Loaded chunk 293/387\n",
      "Loaded chunk 294/387\n",
      "Loaded chunk 295/387\n",
      "Loaded chunk 296/387\n",
      "Loaded chunk 297/387\n",
      "Loaded chunk 298/387\n",
      "Loaded chunk 299/387\n",
      "Loaded chunk 300/387\n",
      "Loaded chunk 301/387\n",
      "Loaded chunk 302/387\n",
      "Loaded chunk 303/387\n",
      "Loaded chunk 304/387\n",
      "Loaded chunk 305/387\n",
      "Loaded chunk 306/387\n",
      "Loaded chunk 307/387\n",
      "Loaded chunk 308/387\n",
      "Loaded chunk 309/387\n",
      "Loaded chunk 310/387\n",
      "Loaded chunk 311/387\n",
      "Loaded chunk 312/387\n",
      "Loaded chunk 313/387\n",
      "Loaded chunk 314/387\n",
      "Loaded chunk 315/387\n",
      "Loaded chunk 316/387\n",
      "Loaded chunk 317/387\n",
      "Loaded chunk 318/387\n",
      "Loaded chunk 319/387\n",
      "Loaded chunk 320/387\n",
      "Loaded chunk 321/387\n",
      "Loaded chunk 322/387\n",
      "Loaded chunk 323/387\n",
      "Loaded chunk 324/387\n",
      "Loaded chunk 325/387\n",
      "Loaded chunk 326/387\n",
      "Loaded chunk 327/387\n",
      "Loaded chunk 328/387\n",
      "Loaded chunk 329/387\n",
      "Loaded chunk 330/387\n",
      "Loaded chunk 331/387\n",
      "Loaded chunk 332/387\n",
      "Loaded chunk 333/387\n",
      "Loaded chunk 334/387\n",
      "Loaded chunk 335/387\n",
      "Loaded chunk 336/387\n",
      "Loaded chunk 337/387\n",
      "Loaded chunk 338/387\n",
      "Loaded chunk 339/387\n",
      "Loaded chunk 340/387\n",
      "Loaded chunk 341/387\n",
      "Loaded chunk 342/387\n",
      "Loaded chunk 343/387\n",
      "Loaded chunk 344/387\n",
      "Loaded chunk 345/387\n",
      "Loaded chunk 346/387\n",
      "Loaded chunk 347/387\n",
      "Loaded chunk 348/387\n",
      "Loaded chunk 349/387\n",
      "Loaded chunk 350/387\n",
      "Loaded chunk 351/387\n",
      "Loaded chunk 352/387\n",
      "Loaded chunk 353/387\n",
      "Loaded chunk 354/387\n",
      "Loaded chunk 355/387\n",
      "Loaded chunk 356/387\n",
      "Loaded chunk 357/387\n",
      "Loaded chunk 358/387\n",
      "Loaded chunk 359/387\n",
      "Loaded chunk 360/387\n",
      "Loaded chunk 361/387\n",
      "Loaded chunk 362/387\n",
      "Loaded chunk 363/387\n",
      "Loaded chunk 364/387\n",
      "Loaded chunk 365/387\n",
      "Loaded chunk 366/387\n",
      "Loaded chunk 367/387\n",
      "Loaded chunk 368/387\n",
      "Loaded chunk 369/387\n",
      "Loaded chunk 370/387\n",
      "Loaded chunk 371/387\n",
      "Loaded chunk 372/387\n",
      "Loaded chunk 373/387\n",
      "Loaded chunk 374/387\n",
      "Loaded chunk 375/387\n",
      "Loaded chunk 376/387\n",
      "Loaded chunk 377/387\n",
      "Loaded chunk 378/387\n",
      "Loaded chunk 379/387\n",
      "Loaded chunk 380/387\n",
      "Loaded chunk 381/387\n",
      "Loaded chunk 382/387\n",
      "Loaded chunk 383/387\n",
      "Loaded chunk 384/387\n",
      "Loaded chunk 385/387\n",
      "Loaded chunk 386/387\n",
      "Loaded chunk 387/387\n",
      "‚úì Successfully loaded 3868396 COVID cases records\n",
      "‚úì Loaded 3868396 COVID cases records\n"
     ]
    }
   ],
   "source": [
    "# Optimized data loading functions\n",
    "def load_large_dataframe_to_sql(df, table_name, engine, schema, chunk_size=10000):\n",
    "    \"\"\"Load large DataFrame to PostgreSQL in chunks to avoid memory issues\"\"\"\n",
    "    # Validate input\n",
    "    validate_dataframe(df, min_rows=1)\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    print(f\"Loading {total_rows} rows in chunks of {chunk_size}...\")\n",
    "    \n",
    "    try:\n",
    "        # First, create the table structure with the first chunk\n",
    "        first_chunk = df.head(1)\n",
    "        first_chunk.to_sql(table_name, engine, schema=schema, if_exists='replace', index=False)\n",
    "        \n",
    "        # Then insert the remaining data in chunks\n",
    "        total_chunks = (total_rows - 1) // chunk_size + 1\n",
    "        for i in range(1, total_rows, chunk_size):\n",
    "            chunk = df.iloc[i:i+chunk_size]\n",
    "            chunk.to_sql(table_name, engine, schema=schema, if_exists='append', index=False)\n",
    "            chunk_num = i // chunk_size + 1\n",
    "            print(f\"Loaded chunk {chunk_num}/{total_chunks}\")\n",
    "        \n",
    "        print(f\"‚úì Successfully loaded {total_rows} COVID cases records\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "def prepare_covid_data(df):\n",
    "    \"\"\"Prepare and clean COVID data for PostgreSQL loading\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert date columns with error handling\n",
    "    date_columns = ['FECHA_ACTUALIZACION', 'FECHA_INGRESO', 'FECHA_SINTOMAS', 'FECHA_DEF']\n",
    "    for col in date_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_datetime(df_clean[col], format='%Y-%m-%d', errors='coerce')\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Load relational data\n",
    "print(\"Loading relational data to PostgreSQL...\")\n",
    "\n",
    "# Prepare COVID cases data for PostgreSQL using helper function\n",
    "df_cases_clean = prepare_covid_data(df_cases)\n",
    "\n",
    "# Replace column names to match PostgreSQL schema\n",
    "column_mapping = {\n",
    "    'ID_REGISTRO': 'id_registro',\n",
    "    'FECHA_ACTUALIZACION': 'fecha_actualizacion',\n",
    "    'FECHA_INGRESO': 'fecha_ingreso',\n",
    "    'FECHA_SINTOMAS': 'fecha_sintomas',\n",
    "    'FECHA_DEF': 'fecha_def',\n",
    "    'ENTIDAD_RES': 'entidad_res',\n",
    "    'MUNICIPIO_RES': 'municipio_res',\n",
    "    'ENTIDAD_UM': 'entidad_um',\n",
    "    'SEXO': 'sexo',\n",
    "    'EDAD': 'edad',\n",
    "    'NACIONALIDAD': 'nacionalidad',\n",
    "    'EMBARAZO': 'embarazo',\n",
    "    'HABLA_LENGUA_INDIG': 'habla_lengua_indig',\n",
    "    'INDIGENA': 'indigena',\n",
    "    'DIABETES': 'diabetes',\n",
    "    'OBESIDAD': 'obesidad',\n",
    "    'HIPERTENSION': 'hipertension',\n",
    "    'RENAL_CRONICA': 'renal_cronica',\n",
    "    'ASMA': 'asma',\n",
    "    'EPOC': 'epoc',\n",
    "    'CARDIOVASCULAR': 'cardiovascular',\n",
    "    'INMUSUPR': 'inmusupr',\n",
    "    'OTRA_COM': 'otra_com',\n",
    "    'CARDIOVASCULAR_AGUDA': 'cardiovascular_aguda',\n",
    "    'OBESIDAD_AGUDA': 'obesidad_aguda',\n",
    "    'DIABETES_AGUDA': 'diabetes_aguda',\n",
    "    'CLASIFICACION_FINAL': 'clasificacion_final',\n",
    "    'MIGRANTE': 'migrante',\n",
    "    'PAIS_ORIGEN': 'pais_origen',\n",
    "    'UCI': 'uci',\n",
    "    'INTUBADO': 'intubado',\n",
    "    'NEUMONIA': 'neumonia',\n",
    "    'TIPO_PACIENTE': 'tipo_paciente',\n",
    "    'RESULTADO_LAB': 'resultado_lab',\n",
    "    'RESULTADO_ANTIGENO': 'resultado_antigeno',\n",
    "    'SECTOR': 'sector',\n",
    "    'YEAR': 'year'\n",
    "}\n",
    "\n",
    "# Rename columns\n",
    "df_cases_clean = df_cases_clean.rename(columns=column_mapping)\n",
    "\n",
    "# Load to PostgreSQL\n",
    "load_large_dataframe_to_sql(df_cases_clean, 'covid_cases', engine, 'relational')\n",
    "print(f\"‚úì Loaded {len(df_cases_clean)} COVID cases records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1dffc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Graph schema created successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create Apache AGE graph schema for time series data\"\"\"\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Enable AGE library and set path\n",
    "    cursor.execute(\"LOAD 'age';\")\n",
    "    cursor.execute(\"SET search_path = ag_catalog, \\\"$user\\\", public;\")\n",
    "\n",
    "    # Drop existing graph if it exists\n",
    "    cursor.execute(\"\"\"\n",
    "    DO $$\n",
    "    BEGIN\n",
    "        IF EXISTS (SELECT 1 FROM ag_catalog.ag_graph WHERE name = 'covid_timeseries') THEN\n",
    "            PERFORM drop_graph('covid_timeseries', true);\n",
    "        END IF;\n",
    "    END$$;\n",
    "    \"\"\")\n",
    "\n",
    "    # Create graph for time series data using Apache AGE\n",
    "    cursor.execute(\"SELECT create_graph('covid_timeseries');\")\n",
    "    \n",
    "    # Create nodes for entities (states)\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT create_vlabel('covid_timeseries', 'entidad');\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create nodes for time points\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT create_vlabel('covid_timeseries', 'fecha');\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create edges for time series relationships\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT create_elabel('covid_timeseries', 'tiene_casos');\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"‚úì Graph schema created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error creating graph schema: {e}\")\n",
    "    raise\n",
    "    \n",
    "finally:\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca71029e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded graph data: 4 series\n"
     ]
    }
   ],
   "source": [
    "# Load catalog data\n",
    "catalogos_file = os.path.join(data_dir, \"relational/240708 Catalogos.xlsx\")\n",
    "cats = pd.read_excel(catalogos_file, sheet_name=None)\n",
    "\n",
    "# Load and transform series data (graph)\n",
    "graph_dir = os.path.join(data_dir, \"graph\")\n",
    "series_files = {\n",
    "    'confirmados': 'Casos_Diarios_Estado_Nacional_Confirmados_20230625.csv',\n",
    "    'defunciones': 'Casos_Diarios_Estado_Nacional_Defunciones_20230625.csv',\n",
    "    'negativos': 'Casos_Diarios_Estado_Nacional_Negativos_20230625.csv',\n",
    "    'sospechosos': 'Casos_Diarios_Estado_Nacional_Sospechosos_20230625.csv'\n",
    "}\n",
    "\n",
    "series_long = {}\n",
    "for name, filename in series_files.items():\n",
    "    file_path = os.path.join(graph_dir, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Transform wide format to long format\n",
    "    id_vars = ['cve_ent', 'poblacion', 'nombre']\n",
    "    date_cols = [col for col in df.columns if col not in id_vars]\n",
    "    \n",
    "    df_long = pd.melt(\n",
    "        df, \n",
    "        id_vars=id_vars, \n",
    "        value_vars=date_cols,\n",
    "        var_name='fecha', \n",
    "        value_name='valor'\n",
    "    )\n",
    "    df_long['metrica'] = name\n",
    "    # FIX: Parse dates with correct format (DD-MM-YYYY)\n",
    "    df_long['fecha'] = pd.to_datetime(\n",
    "        df_long['fecha'].str.strip(),  # remove whitespace\n",
    "        format='%d-%m-%Y',\n",
    "        errors='coerce'\n",
    "    )\n",
    "\n",
    "    # Drop invalid dates\n",
    "    df_long = df_long.dropna(subset=['fecha'])    \n",
    "    series_long[name] = df_long\n",
    "\n",
    "print(f\"‚úì Loaded graph data: {len(series_long)} series\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc57d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 36 entity records\n",
      "Loading time series data to Apache AGE graph...\n",
      "Loading confirmados series data...\n",
      "Loading defunciones series data...\n",
      "Loading negativos series data...\n",
      "Loading sospechosos series data...\n",
      "‚úì Time series data loaded to Apache AGE graph\n"
     ]
    }
   ],
   "source": [
    "# Load entity catalog\n",
    "if 'Cat√°logo de ENTIDADES' in cats:\n",
    "    cat_ent = cats['Cat√°logo de ENTIDADES'].copy()\n",
    "    cat_ent = cat_ent.rename(columns={\n",
    "        'CLAVE_ENTIDAD': 'entidad_id',\n",
    "        'ENTIDAD_FEDERATIVA': 'entidad_nombre',\n",
    "        'ABREVIATURA': 'ent_abbr'\n",
    "    })\n",
    "    cat_ent.to_sql('entidades', engine, schema='relational', if_exists='replace', index=False)\n",
    "    print(f\"‚úì Loaded {len(cat_ent)} entity records\")\n",
    "\n",
    "print(\"Loading time series data to Apache AGE graph...\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    for name, df_series in series_long.items():\n",
    "        print(f\"Loading {name} series data...\")\n",
    "\n",
    "        # Create unique entity nodes\n",
    "        for _, row in df_series.drop_duplicates(['cve_ent', 'nombre', 'poblacion']).iterrows():\n",
    "            cve_ent = str(int(row['cve_ent']))\n",
    "            nombre = row['nombre'].replace(\"'\", \"''\")   # escape quotes\n",
    "            poblacion = int(row['poblacion'])\n",
    "\n",
    "            query = f\"\"\"\n",
    "                SELECT * FROM cypher(\n",
    "                    'covid_timeseries',\n",
    "                    $$\n",
    "                      MERGE (e:entidad {{\n",
    "                        cve_ent: '{cve_ent}',\n",
    "                        nombre: '{nombre}',\n",
    "                        poblacion: {poblacion}\n",
    "                      }})\n",
    "                      RETURN e\n",
    "                    $$\n",
    "                ) AS (e agtype);\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "\n",
    "        # Create date nodes + edges\n",
    "        for _, row in df_series.iterrows():\n",
    "            if pd.notna(row['valor']) and row['valor'] > 0:\n",
    "                cve_ent = str(int(row['cve_ent']))\n",
    "                fecha_str = row['fecha'].strftime('%Y-%m-%d')\n",
    "                valor = int(row['valor'])\n",
    "\n",
    "                query = f\"\"\"\n",
    "                    SELECT * FROM cypher(\n",
    "                        'covid_timeseries',\n",
    "                        $$\n",
    "                          MERGE (f:fecha {{\n",
    "                            fecha: '{fecha_str}',\n",
    "                            metrica: '{name}'\n",
    "                          }})\n",
    "                          WITH f\n",
    "                          MATCH (e:entidad {{cve_ent: '{cve_ent}'}})\n",
    "                          MERGE (e)-[:TIENE_CASOS {{valor: {valor}}}]->(f)\n",
    "                          RETURN e, f\n",
    "                        $$\n",
    "                    ) AS (e agtype, f agtype);\n",
    "                \"\"\"\n",
    "                cursor.execute(query)\n",
    "\n",
    "    print(\"‚úì Time series data loaded to Apache AGE graph\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error loading time series data: {e}\")\n",
    "finally:\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b3d5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Text schema created successfully\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create JSONB schema for news articles text data\"\"\"\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # Create news articles table (instead of Twitter data)\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS text.news_articles (\n",
    "        article_id SERIAL PRIMARY KEY,\n",
    "        title VARCHAR(500),\n",
    "        author VARCHAR(100),\n",
    "        fecha DATE,\n",
    "        categories TEXT[],\n",
    "        content TEXT,\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "    );\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create indexes for performance\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_news_articles_date ON text.news_articles(fecha);\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_news_articles_sentiment ON text.news_articles(categories);\")\n",
    "    cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_news_articles_content ON text.news_articles USING GIN(to_tsvector('spanish', content));\")\n",
    "    \n",
    "    print(\"‚úì Text schema created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error creating text schema: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba45d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse news articles from text files\n",
    "text_dir = os.path.join(data_dir, \"text\")\n",
    "text_files = [f for f in os.listdir(text_dir) if f.endswith('.txt')]\n",
    "\n",
    "news_records = []\n",
    "for text_file in text_files:\n",
    "    file_path = os.path.join(text_dir, text_file)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Parse articles from the text file\n",
    "    articles = content.split('=== ART√çCULO')\n",
    "    \n",
    "    for i, article in enumerate(articles[1:], 1):  # Skip first empty split\n",
    "        # Extract article components\n",
    "        lines = article.strip().split('\\n')\n",
    "        \n",
    "        # Extract title\n",
    "        title_line = lines[0] if lines else \"\"\n",
    "        title = title_line.replace('===', '').strip()\n",
    "        \n",
    "        # Extract metadata\n",
    "        author = \"\"\n",
    "        fecha = \"\"\n",
    "        categories = [] \n",
    "        content_start = 0\n",
    "        \n",
    "        for j, line in enumerate(lines):\n",
    "            if line.startswith('Autor:'):\n",
    "                author = line.replace('Autor:', '').strip()\n",
    "            elif line.startswith('Fecha:'):\n",
    "                fecha = line.replace('Fecha:', '').strip()\n",
    "            elif line.startswith('Categor√≠as:'):\n",
    "                cats_text = line.replace('Categor√≠as:', '').strip()\n",
    "                categories = [cat.strip() for cat in cats_text.split(',')]\n",
    "            elif line.strip() == '--- CONTENIDO ---':\n",
    "                content_start = j + 1\n",
    "                break\n",
    "        \n",
    "        # Extract article content\n",
    "        article_content = '\\n'.join(lines[content_start:]).strip()\n",
    "        \n",
    "        # Parse date\n",
    "        try:\n",
    "            if fecha:\n",
    "                # Handle Spanish month names\n",
    "                fecha_clean = fecha.replace('enero', 'January').replace('febrero', 'February').replace('marzo', 'March').replace('abril', 'April').replace('mayo', 'May').replace('junio', 'June').replace('julio', 'July').replace('agosto', 'August').replace('septiembre', 'September').replace('octubre', 'October').replace('noviembre', 'November').replace('diciembre', 'December')\n",
    "                parsed_date = pd.to_datetime(fecha_clean, errors='coerce')\n",
    "            else:\n",
    "                parsed_date = None\n",
    "        except:\n",
    "            parsed_date = None\n",
    "\n",
    "        news_records.append({\n",
    "            'article_id': f\"{text_file}_{i}\",\n",
    "            'title': title,\n",
    "            'author': author,\n",
    "            'fecha': parsed_date.date() if parsed_date and pd.notna(parsed_date) else None,\n",
    "            'categories': categories,\n",
    "            'content': article_content,\n",
    "            'source_file': text_file,\n",
    "            'created_at': pd.Timestamp.now()\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4587393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Loaded 802 news articles\n"
     ]
    }
   ],
   "source": [
    "# Load text data (news articles) to JSONB table\n",
    "if news_records:\n",
    "    df_news_final = pd.DataFrame(news_records)\n",
    "    df_news_final.to_sql('news_articles', engine, schema='text', if_exists='replace', index=False, method='multi')\n",
    "    print(f\"‚úì Loaded {len(df_news_final)} news articles\")\n",
    "else:\n",
    "    print(\"‚ö† No news articles were parsed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d543a449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Federation schema created successfully with graph integration\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Create unified views across schemas including graph data\"\"\"\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    # 1. Extract Graph Data (optimized: remove unnecessary casts)\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW federation.graph_data_extracted AS\n",
    "    SELECT \n",
    "        entidad_id::text AS entidad_id,\n",
    "        entidad_nombre::text AS entidad_nombre,\n",
    "        fecha::text::date AS fecha,\n",
    "        metrica::text AS metrica,\n",
    "        valor::integer AS valor,\n",
    "        poblacion::integer AS poblacion\n",
    "    FROM cypher('covid_timeseries', $$\n",
    "        MATCH (e:entidad)-[r:TIENE_CASOS]->(f:fecha)\n",
    "        RETURN \n",
    "            e.cve_ent       AS entidad_id,\n",
    "            e.nombre        AS entidad_nombre,\n",
    "            e.poblacion     AS poblacion,\n",
    "            f.fecha         AS fecha,\n",
    "            f.metrica       AS metrica,\n",
    "            r.valor         AS valor\n",
    "    $$) AS (\n",
    "        entidad_id agtype,\n",
    "        entidad_nombre agtype,\n",
    "        poblacion agtype,\n",
    "        fecha agtype,\n",
    "        metrica agtype,\n",
    "        valor agtype\n",
    "    )\n",
    "    WHERE entidad_id IS NOT NULL AND fecha IS NOT NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    # 2. Unified data (relational + graph + text) - optimized casts\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW federation.unified_covid_data AS\n",
    "    -- Relational data\n",
    "    SELECT \n",
    "        'relational' as data_source,\n",
    "        c.fecha_actualizacion::date as fecha,\n",
    "        'casos_clinicos' as metrica,\n",
    "        c.clasificacion_final::text as valor,\n",
    "        json_build_object(\n",
    "            'tipo', 'relational',\n",
    "            'edad', c.edad,\n",
    "            'sexo', c.sexo,\n",
    "            'resultado_lab', c.resultado_lab,\n",
    "            'fecha_sintomas', c.fecha_sintomas,\n",
    "            'fecha_def', c.fecha_def\n",
    "        ) as metadata\n",
    "    FROM relational.covid_cases c\n",
    "    LEFT JOIN relational.entidades e ON c.entidad_res = e.entidad_id\n",
    "    WHERE c.clasificacion_final IS NOT NULL\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Graph data\n",
    "    SELECT \n",
    "        'graph' as data_source,\n",
    "        g.fecha as fecha,\n",
    "        g.metrica,\n",
    "        g.valor::text as valor,\n",
    "        json_build_object(\n",
    "            'tipo', 'graph',\n",
    "            'poblacion', g.poblacion,\n",
    "            'metrica_tipo', g.metrica\n",
    "        ) as metadata\n",
    "    FROM federation.graph_data_extracted g\n",
    "    WHERE g.valor > 0\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    -- Text data\n",
    "    SELECT \n",
    "        'text' as data_source,\n",
    "        n.fecha as fecha,\n",
    "        'news_articles' as metrica,\n",
    "        n.article_id::text as valor,\n",
    "        json_build_object(\n",
    "            'tipo', 'text',\n",
    "            'title', n.title,\n",
    "            'author', n.author,\n",
    "            'categories', n.categories,\n",
    "            'content_preview', LEFT(n.content, 200)\n",
    "        ) as metadata\n",
    "    FROM text.news_articles n\n",
    "    WHERE n.fecha IS NOT NULL;\n",
    "    \"\"\")\n",
    "\n",
    "    # 3. Comprehensive correlation\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW federation.comprehensive_correlation AS\n",
    "    WITH daily_relational AS (\n",
    "        SELECT \n",
    "            c.fecha_actualizacion as fecha,\n",
    "            c.entidad_res as entidad_id,\n",
    "            COUNT(c.id_registro) as daily_cases,\n",
    "            COUNT(CASE WHEN c.clasificacion_final = 1 THEN 1 END) as confirmed_cases,\n",
    "            COUNT(CASE WHEN c.clasificacion_final = 2 THEN 1 END) as suspected_cases,\n",
    "            COUNT(CASE WHEN c.fecha_def IS NOT NULL THEN 1 END) as deaths\n",
    "        FROM relational.covid_cases c\n",
    "        WHERE c.fecha_actualizacion IS NOT NULL\n",
    "        GROUP BY c.fecha_actualizacion, c.entidad_res\n",
    "    ),\n",
    "    daily_graph AS (\n",
    "        SELECT \n",
    "            fecha,\n",
    "            entidad_id::bigint as entidad_id,\n",
    "            entidad_nombre,\n",
    "            poblacion,\n",
    "            SUM(CASE WHEN metrica = 'confirmados' THEN valor ELSE 0 END) as graph_confirmados,\n",
    "            SUM(CASE WHEN metrica = 'defunciones' THEN valor ELSE 0 END) as graph_defunciones,\n",
    "            SUM(CASE WHEN metrica = 'negativos' THEN valor ELSE 0 END) as graph_negativos,\n",
    "            SUM(CASE WHEN metrica = 'sospechosos' THEN valor ELSE 0 END) as graph_sospechosos\n",
    "        FROM federation.graph_data_extracted\n",
    "        WHERE valor > 0\n",
    "        GROUP BY fecha, entidad_id::bigint, entidad_nombre, poblacion\n",
    "    ),\n",
    "    daily_text AS (\n",
    "        SELECT \n",
    "            fecha,\n",
    "            COUNT(article_id) as news_articles_count,\n",
    "            STRING_AGG(DISTINCT category, ', ') as news_categories,\n",
    "            STRING_AGG(DISTINCT author, ', ') as authors\n",
    "        FROM text.news_articles,\n",
    "            LATERAL unnest(categories::text[]) as category\n",
    "        WHERE fecha IS NOT NULL\n",
    "        GROUP BY fecha\n",
    "    )\n",
    "    SELECT \n",
    "        COALESCE(r.fecha, g.fecha, t.fecha) as fecha,\n",
    "        g.entidad_id,\n",
    "        g.entidad_nombre,\n",
    "        g.poblacion,\n",
    "        COALESCE(r.daily_cases, 0) as relational_cases,\n",
    "        COALESCE(r.confirmed_cases, 0) as relational_confirmed,\n",
    "        COALESCE(r.suspected_cases, 0) as relational_suspected,\n",
    "        COALESCE(r.deaths, 0) as relational_deaths,\n",
    "        COALESCE(g.graph_confirmados, 0) as graph_confirmados,\n",
    "        COALESCE(g.graph_defunciones, 0) as graph_defunciones,\n",
    "        COALESCE(g.graph_negativos, 0) as graph_negativos,\n",
    "        COALESCE(g.graph_sospechosos, 0) as graph_sospechosos,\n",
    "        COALESCE(t.news_articles_count, 0) as news_articles_count,\n",
    "        t.news_categories,\n",
    "        t.authors,\n",
    "        CASE \n",
    "            WHEN g.poblacion > 0 THEN (COALESCE(r.confirmed_cases, 0) * 100000.0 / g.poblacion)\n",
    "            ELSE 0 \n",
    "        END as tasa_confirmados_100k,\n",
    "        CASE \n",
    "            WHEN g.poblacion > 0 THEN (COALESCE(r.deaths, 0) * 100000.0 / g.poblacion)\n",
    "            ELSE 0 \n",
    "        END as tasa_defunciones_100k\n",
    "    FROM daily_relational r\n",
    "    FULL OUTER JOIN daily_graph g ON r.fecha = g.fecha AND r.entidad_id = g.entidad_id\n",
    "    FULL OUTER JOIN daily_text t ON COALESCE(r.fecha, g.fecha) = t.fecha\n",
    "    ORDER BY COALESCE(r.fecha, g.fecha, t.fecha) DESC, g.entidad_id;\n",
    "    \"\"\")\n",
    "\n",
    "    # 4. Graph analysis (optimized)\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW federation.graph_analysis AS\n",
    "    SELECT \n",
    "        entidad_id,\n",
    "        entidad_nombre,\n",
    "        poblacion,\n",
    "        fecha,\n",
    "        metrica,\n",
    "        valor,\n",
    "        CASE \n",
    "            WHEN poblacion > 0 THEN (valor * 100000.0 / poblacion)\n",
    "            ELSE 0 \n",
    "        END as tasa_100k,\n",
    "        SUM(valor) OVER (\n",
    "            PARTITION BY entidad_id, metrica \n",
    "            ORDER BY fecha \n",
    "            ROWS UNBOUNDED PRECEDING\n",
    "        ) as valor_acumulado,\n",
    "        AVG(valor) OVER (\n",
    "            PARTITION BY entidad_id, metrica \n",
    "            ORDER BY fecha \n",
    "            ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "        ) as promedio_7_dias\n",
    "    FROM federation.graph_data_extracted\n",
    "    WHERE valor > 0\n",
    "    ORDER BY entidad_id, fecha, metrica;\n",
    "    \"\"\")\n",
    "\n",
    "    print(\"‚úì Federation schema created successfully with graph integration\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error creating federation schema: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b84be9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COVID DATA FEDERATION SCHEMA ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "1. RELATIONAL DATA SOURCE (relational.covid_cases)\n",
      "--------------------------------------------------\n",
      "Key columns used for federation:\n",
      "  ‚Ä¢ fecha_actualizacion: timestamp without time zone (NULL)\n",
      "  ‚Ä¢ sexo: bigint (NULL)\n",
      "  ‚Ä¢ entidad_res: bigint (NULL)\n",
      "  ‚Ä¢ fecha_sintomas: timestamp without time zone (NULL)\n",
      "  ‚Ä¢ fecha_def: timestamp without time zone (NULL)\n",
      "  ‚Ä¢ edad: bigint (NULL)\n",
      "  ‚Ä¢ resultado_lab: bigint (NULL)\n",
      "  ‚Ä¢ clasificacion_final: bigint (NULL)\n",
      "\n",
      "2. GRAPH DATA SOURCE (federation.graph_data_extracted)\n",
      "--------------------------------------------------\n",
      "Key columns used for federation:\n",
      "  ‚Ä¢ entidad_id: text (NULL)\n",
      "  ‚Ä¢ entidad_nombre: text (NULL)\n",
      "  ‚Ä¢ fecha: date (NULL)\n",
      "  ‚Ä¢ metrica: text (NULL)\n",
      "  ‚Ä¢ valor: integer (NULL)\n",
      "  ‚Ä¢ poblacion: integer (NULL)\n",
      "\n",
      "3. TEXT DATA SOURCE (text.news_articles)\n",
      "--------------------------------------------------\n",
      "Key columns used for federation:\n",
      "  ‚Ä¢ article_id: text (NULL)\n",
      "  ‚Ä¢ title: text (NULL)\n",
      "  ‚Ä¢ author: text (NULL)\n",
      "  ‚Ä¢ fecha: date (NULL)\n",
      "  ‚Ä¢ categories: text (NULL)\n",
      "  ‚Ä¢ content: text (NULL)\n",
      "\n",
      "4. FEDERATION SCHEMA STRUCTURE\n",
      "--------------------------------------------------\n",
      "Unified View: federation.unified_covid_data\n",
      "Columns:\n",
      "  ‚Ä¢ data_source: text (relational|graph|text)\n",
      "  ‚Ä¢ fecha: date (unified date field)\n",
      "  ‚Ä¢ metrica: text (metric type)\n",
      "  ‚Ä¢ valor: text (metric value - cast to text for consistency)\n",
      "  ‚Ä¢ metadata: jsonb (source-specific attributes)\n",
      "\n",
      "Comprehensive View: federation.comprehensive_correlation\n",
      "Columns:\n",
      "  ‚Ä¢ fecha: date\n",
      "  ‚Ä¢ entidad_id: bigint (unified entity ID)\n",
      "  ‚Ä¢ entidad_nombre: text\n",
      "  ‚Ä¢ poblacion: integer\n",
      "  ‚Ä¢ relational_cases: bigint\n",
      "  ‚Ä¢ relational_confirmed: bigint\n",
      "  ‚Ä¢ relational_suspected: bigint\n",
      "  ‚Ä¢ relational_deaths: bigint\n",
      "  ‚Ä¢ graph_confirmados: bigint\n",
      "  ‚Ä¢ graph_defunciones: bigint\n",
      "  ‚Ä¢ graph_negativos: bigint\n",
      "  ‚Ä¢ graph_sospechosos: bigint\n",
      "  ‚Ä¢ news_articles_count: bigint\n",
      "  ‚Ä¢ news_categories: text\n",
      "  ‚Ä¢ authors: text\n",
      "  ‚Ä¢ tasa_confirmados_100k: numeric\n",
      "  ‚Ä¢ tasa_defunciones_100k: numeric\n",
      "\n",
      "Graph Analysis View: federation.graph_analysis\n",
      "Columns:\n",
      "  ‚Ä¢ entidad_id: text\n",
      "  ‚Ä¢ entidad_nombre: text\n",
      "  ‚Ä¢ poblacion: integer\n",
      "  ‚Ä¢ fecha: date\n",
      "  ‚Ä¢ metrica: text\n",
      "  ‚Ä¢ valor: integer\n",
      "  ‚Ä¢ tasa_100k: numeric\n",
      "  ‚Ä¢ valor_acumulado: bigint\n",
      "  ‚Ä¢ promedio_7_dias: numeric\n",
      "\n",
      "5. DATA UNIFICATION STRATEGY\n",
      "--------------------------------------------------\n",
      "Unification Keys:\n",
      "  ‚Ä¢ Date: fecha_actualizacion (relational) ‚Üî fecha (graph/text)\n",
      "  ‚Ä¢ Entity: entidad_res (relational) ‚Üî entidad_id (graph)\n",
      "  ‚Ä¢ Metrics: clasificacion_final (relational) ‚Üî metrica (graph)\n",
      "\n",
      "Data Type Harmonization:\n",
      "  ‚Ä¢ All valor fields cast to TEXT for UNION compatibility\n",
      "  ‚Ä¢ entidad_id cast to BIGINT for JOIN compatibility\n",
      "  ‚Ä¢ Date fields standardized to DATE type\n",
      "\n",
      "Metadata Enrichment:\n",
      "  ‚Ä¢ Relational: edad, sexo, resultado_lab, fecha_sintomas, fecha_def\n",
      "  ‚Ä¢ Graph: poblacion, metrica_tipo\n",
      "  ‚Ä¢ Text: title, author, categories, content_preview\n",
      "\n",
      "6. SAMPLE DATA PREVIEW\n",
      "--------------------------------------------------\n",
      "\n",
      "Unified Data Sample (5 records):\n",
      "  1. Source: relational, Date: 2021-10-31, Metric: casos_clinicos, Value: 3\n",
      "     Metadata: {'tipo': 'relational', 'edad': 55, 'sexo': 1, 'resultado_lab': 1, 'fecha_sintomas': '2020-11-30T00:00:00', 'fecha_def': None}\n",
      "  2. Source: relational, Date: 2021-10-31, Metric: casos_clinicos, Value: 6\n",
      "     Metadata: {'tipo': 'relational', 'edad': 59, 'sexo': 2, 'resultado_lab': 97, 'fecha_sintomas': '2020-02-18T00:00:00', 'fecha_def': None}\n",
      "  3. Source: relational, Date: 2021-10-31, Metric: casos_clinicos, Value: 7\n",
      "     Metadata: {'tipo': 'relational', 'edad': 42, 'sexo': 1, 'resultado_lab': 2, 'fecha_sintomas': '2020-04-18T00:00:00', 'fecha_def': None}\n",
      "  4. Source: relational, Date: 2021-10-31, Metric: casos_clinicos, Value: 3\n",
      "     Metadata: {'tipo': 'relational', 'edad': 35, 'sexo': 2, 'resultado_lab': 1, 'fecha_sintomas': '2020-07-20T00:00:00', 'fecha_def': None}\n",
      "  5. Source: relational, Date: 2021-10-31, Metric: casos_clinicos, Value: 7\n",
      "     Metadata: {'tipo': 'relational', 'edad': 51, 'sexo': 2, 'resultado_lab': 2, 'fecha_sintomas': '2020-08-17T00:00:00', 'fecha_def': None}\n",
      "\n",
      "Comprehensive Correlation Sample (3 records):\n",
      "  1. Date: 2023-06-24 00:00:00, Entity: Nacional\n",
      "     Relational Confirmed: 0, Graph Confirmed: 15, News: 0\n",
      "  2. Date: 2023-06-24 00:00:00, Entity: DISTRITO FEDERAL\n",
      "     Relational Confirmed: 0, Graph Confirmed: 4, News: 0\n",
      "  3. Date: 2023-06-24 00:00:00, Entity: GUERRERO\n",
      "     Relational Confirmed: 0, Graph Confirmed: 2, News: 0\n",
      "\n",
      "================================================================================\n",
      "FEDERATION SCHEMA ANALYSIS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "7. PERFORMANCE OPTIMIZATIONS APPLIED\n",
      "--------------------------------------------------\n",
      "‚úì Removed unnecessary type casts in federation views\n",
      "‚úì Optimized data types (INTEGER ‚Üí BIGINT) to match actual data\n",
      "‚úì Improved error handling with try-catch blocks\n",
      "‚úì Added data validation and quality checks\n",
      "‚úì Created helper functions for better code organization\n",
      "‚úì Optimized SQL queries for better performance\n",
      "‚úì Added comprehensive data quality reporting\n",
      "‚úì Improved connection management and error recovery\n",
      "\n",
      "================================================================================\n",
      "OPTIMIZATION SUMMARY COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Data Source Structure Analysis and Federation Schema Overview\n",
    "cursor = conn.cursor()\n",
    "\n",
    "try:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COVID DATA FEDERATION SCHEMA ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 1. Relational Data Structure\n",
    "    print(\"\\n1. RELATIONAL DATA SOURCE (relational.covid_cases)\")\n",
    "    print(\"-\" * 50)\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        data_type,\n",
    "        is_nullable,\n",
    "        column_default\n",
    "    FROM information_schema.columns \n",
    "    WHERE table_schema = 'relational' \n",
    "    AND table_name = 'covid_cases'\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\")\n",
    "    \n",
    "    relational_cols = cursor.fetchall()\n",
    "    print(\"Key columns used for federation:\")\n",
    "    for col in relational_cols:\n",
    "        if col[0] in ['fecha_actualizacion', 'entidad_res', 'clasificacion_final', 'edad', 'sexo', 'resultado_lab', 'fecha_sintomas', 'fecha_def']:\n",
    "            print(f\"  ‚Ä¢ {col[0]}: {col[1]} ({'NULL' if col[2] == 'YES' else 'NOT NULL'})\")\n",
    "    \n",
    "    # 2. Graph Data Structure\n",
    "    print(\"\\n2. GRAPH DATA SOURCE (federation.graph_data_extracted)\")\n",
    "    print(\"-\" * 50)\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        data_type,\n",
    "        is_nullable\n",
    "    FROM information_schema.columns \n",
    "    WHERE table_schema = 'federation' \n",
    "    AND table_name = 'graph_data_extracted'\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\")\n",
    "    \n",
    "    graph_cols = cursor.fetchall()\n",
    "    print(\"Key columns used for federation:\")\n",
    "    for col in graph_cols:\n",
    "        print(f\"  ‚Ä¢ {col[0]}: {col[1]} ({'NULL' if col[2] == 'YES' else 'NOT NULL'})\")\n",
    "    \n",
    "    # 3. Text Data Structure\n",
    "    print(\"\\n3. TEXT DATA SOURCE (text.news_articles)\")\n",
    "    print(\"-\" * 50)\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        data_type,\n",
    "        is_nullable\n",
    "    FROM information_schema.columns \n",
    "    WHERE table_schema = 'text' \n",
    "    AND table_name = 'news_articles'\n",
    "    ORDER BY ordinal_position;\n",
    "    \"\"\")\n",
    "    \n",
    "    text_cols = cursor.fetchall()\n",
    "    print(\"Key columns used for federation:\")\n",
    "    for col in text_cols:\n",
    "        if col[0] in ['fecha', 'article_id', 'title', 'author', 'categories', 'content']:\n",
    "            print(f\"  ‚Ä¢ {col[0]}: {col[1]} ({'NULL' if col[2] == 'YES' else 'NOT NULL'})\")\n",
    "    \n",
    "    # 4. Federation Schema Structure\n",
    "    print(\"\\n4. FEDERATION SCHEMA STRUCTURE\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Unified View: federation.unified_covid_data\")\n",
    "    print(\"Columns:\")\n",
    "    print(\"  ‚Ä¢ data_source: text (relational|graph|text)\")\n",
    "    print(\"  ‚Ä¢ fecha: date (unified date field)\")\n",
    "    print(\"  ‚Ä¢ metrica: text (metric type)\")\n",
    "    print(\"  ‚Ä¢ valor: text (metric value - cast to text for consistency)\")\n",
    "    print(\"  ‚Ä¢ metadata: jsonb (source-specific attributes)\")\n",
    "    \n",
    "    print(\"\\nComprehensive View: federation.comprehensive_correlation\")\n",
    "    print(\"Columns:\")\n",
    "    print(\"  ‚Ä¢ fecha: date\")\n",
    "    print(\"  ‚Ä¢ entidad_id: bigint (unified entity ID)\")\n",
    "    print(\"  ‚Ä¢ entidad_nombre: text\")\n",
    "    print(\"  ‚Ä¢ poblacion: integer\")\n",
    "    print(\"  ‚Ä¢ relational_cases: bigint\")\n",
    "    print(\"  ‚Ä¢ relational_confirmed: bigint\")\n",
    "    print(\"  ‚Ä¢ relational_suspected: bigint\")\n",
    "    print(\"  ‚Ä¢ relational_deaths: bigint\")\n",
    "    print(\"  ‚Ä¢ graph_confirmados: bigint\")\n",
    "    print(\"  ‚Ä¢ graph_defunciones: bigint\")\n",
    "    print(\"  ‚Ä¢ graph_negativos: bigint\")\n",
    "    print(\"  ‚Ä¢ graph_sospechosos: bigint\")\n",
    "    print(\"  ‚Ä¢ news_articles_count: bigint\")\n",
    "    print(\"  ‚Ä¢ news_categories: text\")\n",
    "    print(\"  ‚Ä¢ authors: text\")\n",
    "    print(\"  ‚Ä¢ tasa_confirmados_100k: numeric\")\n",
    "    print(\"  ‚Ä¢ tasa_defunciones_100k: numeric\")\n",
    "    \n",
    "    print(\"\\nGraph Analysis View: federation.graph_analysis\")\n",
    "    print(\"Columns:\")\n",
    "    print(\"  ‚Ä¢ entidad_id: text\")\n",
    "    print(\"  ‚Ä¢ entidad_nombre: text\")\n",
    "    print(\"  ‚Ä¢ poblacion: integer\")\n",
    "    print(\"  ‚Ä¢ fecha: date\")\n",
    "    print(\"  ‚Ä¢ metrica: text\")\n",
    "    print(\"  ‚Ä¢ valor: integer\")\n",
    "    print(\"  ‚Ä¢ tasa_100k: numeric\")\n",
    "    print(\"  ‚Ä¢ valor_acumulado: bigint\")\n",
    "    print(\"  ‚Ä¢ promedio_7_dias: numeric\")\n",
    "    \n",
    "    # 5. Data Unification Strategy\n",
    "    print(\"\\n5. DATA UNIFICATION STRATEGY\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"Unification Keys:\")\n",
    "    print(\"  ‚Ä¢ Date: fecha_actualizacion (relational) ‚Üî fecha (graph/text)\")\n",
    "    print(\"  ‚Ä¢ Entity: entidad_res (relational) ‚Üî entidad_id (graph)\")\n",
    "    print(\"  ‚Ä¢ Metrics: clasificacion_final (relational) ‚Üî metrica (graph)\")\n",
    "    \n",
    "    print(\"\\nData Type Harmonization:\")\n",
    "    print(\"  ‚Ä¢ All valor fields cast to TEXT for UNION compatibility\")\n",
    "    print(\"  ‚Ä¢ entidad_id cast to BIGINT for JOIN compatibility\")\n",
    "    print(\"  ‚Ä¢ Date fields standardized to DATE type\")\n",
    "    \n",
    "    print(\"\\nMetadata Enrichment:\")\n",
    "    print(\"  ‚Ä¢ Relational: edad, sexo, resultado_lab, fecha_sintomas, fecha_def\")\n",
    "    print(\"  ‚Ä¢ Graph: poblacion, metrica_tipo\")\n",
    "    print(\"  ‚Ä¢ Text: title, author, categories, content_preview\")\n",
    "    \n",
    "    # 6. Sample Data Preview\n",
    "    print(\"\\n6. SAMPLE DATA PREVIEW\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"\\nUnified Data Sample (5 records):\")\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        data_source,\n",
    "        fecha,\n",
    "        metrica,\n",
    "        valor,\n",
    "        metadata\n",
    "    FROM federation.unified_covid_data \n",
    "    LIMIT 5;\n",
    "    \"\"\")\n",
    "    \n",
    "    unified_sample = cursor.fetchall()\n",
    "    for i, row in enumerate(unified_sample, 1):\n",
    "        print(f\"  {i}. Source: {row[0]}, Date: {row[1]}, Metric: {row[2]}, Value: {row[3]}\")\n",
    "        print(f\"     Metadata: {row[4]}\")\n",
    "    \n",
    "    print(\"\\nComprehensive Correlation Sample (3 records):\")\n",
    "    cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        fecha,\n",
    "        entidad_nombre,\n",
    "        relational_confirmed,\n",
    "        graph_confirmados,\n",
    "        news_articles_count\n",
    "    FROM federation.comprehensive_correlation \n",
    "    WHERE relational_confirmed > 0 OR graph_confirmados > 0\n",
    "    LIMIT 3;\n",
    "    \"\"\")\n",
    "    \n",
    "    correlation_sample = cursor.fetchall()\n",
    "    for i, row in enumerate(correlation_sample, 1):\n",
    "        print(f\"  {i}. Date: {row[0]}, Entity: {row[1]}\")\n",
    "        print(f\"     Relational Confirmed: {row[2]}, Graph Confirmed: {row[3]}, News: {row[4]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FEDERATION SCHEMA ANALYSIS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 7. Performance Optimizations Summary\n",
    "    print(\"\\n7. PERFORMANCE OPTIMIZATIONS APPLIED\")\n",
    "    print(\"-\" * 50)\n",
    "    print(\"‚úì Removed unnecessary type casts in federation views\")\n",
    "    print(\"‚úì Optimized data types (INTEGER ‚Üí BIGINT) to match actual data\")\n",
    "    print(\"‚úì Improved error handling with try-catch blocks\")\n",
    "    print(\"‚úì Added data validation and quality checks\")\n",
    "    print(\"‚úì Created helper functions for better code organization\")\n",
    "    print(\"‚úì Optimized SQL queries for better performance\")\n",
    "    print(\"‚úì Added comprehensive data quality reporting\")\n",
    "    print(\"‚úì Improved connection management and error recovery\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"OPTIMIZATION SUMMARY COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö† Error in schema analysis: {e}\")\n",
    "    raise\n",
    "finally:\n",
    "    cursor.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
