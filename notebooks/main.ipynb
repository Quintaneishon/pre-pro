{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d076bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:52:16.610896Z",
     "start_time": "2025-09-17T22:51:40.730347Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install openpyxl\n",
    "!pip install duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Fuentes de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b34b9a7551397",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Estructurada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52acb80c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:52:22.723643Z",
     "start_time": "2025-09-17T22:52:16.615402Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the data directory path\n",
    "data_dir = \"../data/relational\"\n",
    "\n",
    "print(\"=== EXAMINING XLSX FILES ===\")\n",
    "print()\n",
    "\n",
    "# Read the Catalogos.xlsx file\n",
    "print(\"1. CATALOGOS.XLSX:\")\n",
    "print(\"-\" * 50)\n",
    "catalogos_file = os.path.join(data_dir, \"240708 Catalogos.xlsx\")\n",
    "try:\n",
    "    # Read all sheets from the Excel file\n",
    "    catalogos_sheets = pd.read_excel(catalogos_file, sheet_name=None)\n",
    "    print(f\"Number of sheets: {len(catalogos_sheets)}\")\n",
    "    print(f\"Sheet names: {list(catalogos_sheets.keys())}\")\n",
    "    print()\n",
    "    \n",
    "    for sheet_name, df in catalogos_sheets.items():\n",
    "        print(f\"Sheet: {sheet_name}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(\"First 5 rows:\")\n",
    "        print(df.head())\n",
    "        print(\"-\" * 30)\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error reading catalogos file: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print()\n",
    "\n",
    "# Read the Descriptores.xlsx file\n",
    "print(\"2. DESCRIPTORES.XLSX:\")\n",
    "print(\"-\" * 50)\n",
    "descriptores_file = os.path.join(data_dir, \"240708 Descriptores_.xlsx\")\n",
    "try:\n",
    "    # Read all sheets from the Excel file\n",
    "    descriptores_sheets = pd.read_excel(descriptores_file, sheet_name=None)\n",
    "    print(f\"Number of sheets: {len(descriptores_sheets)}\")\n",
    "    print(f\"Sheet names: {list(descriptores_sheets.keys())}\")\n",
    "    print()\n",
    "    \n",
    "    for sheet_name, df in descriptores_sheets.items():\n",
    "        print(f\"Sheet: {sheet_name}\")\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print(\"First 5 rows:\")\n",
    "        print(df.head())\n",
    "        print(\"-\" * 30)\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error reading descriptores file: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print()\n",
    "\n",
    "# Examine the CSV file structure\n",
    "print(\"3. COVID19MEXICO202.CSV STRUCTURE:\")\n",
    "print(\"-\" * 50)\n",
    "csv_file = os.path.join(data_dir, \"COVID19MEXICO2020.csv\")\n",
    "try:\n",
    "    # Read just the first few rows to understand structure\n",
    "    df_csv = pd.read_csv(csv_file, nrows=10)\n",
    "    print(f\"CSV Shape (first 10 rows): {df_csv.shape}\")\n",
    "    print(f\"CSV Columns: {list(df_csv.columns)}\")\n",
    "    print(\"First 5 rows of CSV:\")\n",
    "    print(df_csv.head())\n",
    "    print()\n",
    "    \n",
    "    # Get info about the full CSV\n",
    "    df_info = pd.read_csv(csv_file)\n",
    "    print(f\"Full CSV Shape: {df_info.shape}\")\n",
    "    print(f\"Data types:\")\n",
    "    print(df_info.dtypes)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV file: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8c630f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:52:22.776059Z",
     "start_time": "2025-09-17T22:52:22.726188Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the graph data directory path\n",
    "graph_dir = \"../data/graph\"\n",
    "\n",
    "print(\"GRAPH DATA FILES:\")\n",
    "print(\"-\" * 50)\n",
    "graph_files = [\n",
    "    \"Casos_Diarios_Estado_Nacional_Confirmados_20230625.csv\",\n",
    "    \"Casos_Diarios_Estado_Nacional_Defunciones_20230625.csv\", \n",
    "    \"Casos_Diarios_Estado_Nacional_Negativos_20230625.csv\",\n",
    "    \"Casos_Diarios_Estado_Nacional_Sospechosos_20230625.csv\"\n",
    "]\n",
    "\n",
    "for file in graph_files:\n",
    "    print(f\"• {file}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Analyze each file\n",
    "for i, file in enumerate(graph_files, 1):\n",
    "    print(f\"{i}. {file.upper()}:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    file_path = os.path.join(graph_dir, file)\n",
    "    try:\n",
    "        # Read the CSV file\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"Shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        print()\n",
    "        \n",
    "        # Show first few rows\n",
    "        print(\"First 3 rows:\")\n",
    "        print(df.head(3))\n",
    "        print()\n",
    "        \n",
    "        # Analyze the data structure\n",
    "        print(\"Data Analysis:\")\n",
    "        print(f\"• Number of states: {len(df)}\")\n",
    "        print(f\"• Date range: {df.columns[3]} to {df.columns[-1]}\")\n",
    "        print(f\"• Total days covered: {len(df.columns) - 3}\")\n",
    "        print(f\"• Population column: {df.columns[1]}\")\n",
    "        print(f\"• State name column: {df.columns[2]}\")\n",
    "        print()\n",
    "        \n",
    "        # Show some statistics\n",
    "        print(\"Sample Statistics:\")\n",
    "        numeric_cols = df.columns[3:]  # All date columns\n",
    "        sample_dates = numeric_cols[:5]  # First 5 date columns\n",
    "        print(f\"Sample dates: {list(sample_dates)}\")\n",
    "        print(f\"Sample values for first state (Aguascalientes):\")\n",
    "        for col in sample_dates:\n",
    "            print(f\"  {col}: {df.iloc[0][col]}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4285b998",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:52:37.763387Z",
     "start_time": "2025-09-17T22:52:22.779870Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the text data directory path\n",
    "text_dir = \"../data/text\"\n",
    "\n",
    "print(\"DATA FILES:\")\n",
    "print(\"-\" * 50)\n",
    "text_files = [\n",
    "    \"data_descriptor.txt\",\n",
    "    \"mexico_1-003.tsv\", \n",
    "    \"mexico_2-001.tsv\",\n",
    "    \"mexico_3-004.tsv\"\n",
    "]\n",
    "\n",
    "for file in text_files:\n",
    "    print(f\"• {file}\")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Analyze the descriptor file\n",
    "print(\"1. DATA DESCRIPTOR ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"The descriptor file explains the structure of the TSV files:\")\n",
    "print()\n",
    "print(\"TSV FILES CONTAIN TWITTER DATA WITH THE FOLLOWING ATTRIBUTES:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Read and display the descriptor content\n",
    "descriptor_path = os.path.join(text_dir, \"data_descriptor.txt\")\n",
    "with open(descriptor_path, 'r') as f:\n",
    "    descriptor_content = f.read()\n",
    "    print(descriptor_content)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "\n",
    "# Analyze TSV files (without loading them fully due to size)\n",
    "print(\"2. TSV FILES ANALYSIS:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "tsv_files = [\"mexico_1-003.tsv\", \"mexico_2-001.tsv\", \"mexico_3-004.tsv\"]\n",
    "\n",
    "for i, file in enumerate(tsv_files, 1):\n",
    "    print(f\"{i}. {file.upper()}:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    file_path = os.path.join(text_dir, file)\n",
    "    \n",
    "    # Get file size in bytes\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    \n",
    "    # Count lines efficiently\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            line_count = sum(1 for line in f)\n",
    "    except:\n",
    "        line_count = \"Unknown (file too large)\"\n",
    "    \n",
    "    print(f\"• File size: {file_size_mb:.1f} MB\")\n",
    "    print(f\"• Total rows: {line_count:,}\" if isinstance(line_count, int) else f\"• Total rows: {line_count}\")\n",
    "    print(f\"• Data type: Twitter/X social media posts\")\n",
    "    print(f\"• Time period: 2020-2023 (COVID-19 era)\")\n",
    "    print(f\"• Language: Mixed (primarily Spanish and English)\")\n",
    "    print()\n",
    "\n",
    "print(\"TSV FILES STRUCTURE SUMMARY:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Each TSV file contains Twitter data with these key columns:\")\n",
    "print(\"• tweet_id: Unique tweet identifier\")\n",
    "print(\"• date_time: When the tweet was posted\")\n",
    "print(\"• lang: Language code (en, es, etc.)\")\n",
    "print(\"• user_id: Author identifier\")\n",
    "print(\"• tweet_text_*: Tweet content and extracted entities\")\n",
    "print(\"• sentiment_label: -1 (negative), 0 (neutral), 1 (positive)\")\n",
    "print(\"• sentiment_conf: Confidence score for sentiment\")\n",
    "print(\"• geo_*: GPS coordinates and location data\")\n",
    "print(\"• place_*: Twitter place tags\")\n",
    "print(\"• user_loc_*: User location information\")\n",
    "print(\"• *_toponyms: Named entities and locations extracted from text\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4538461",
   "metadata": {},
   "source": [
    "## Esquema canonico y mappeo de fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6164a40c",
   "metadata": {},
   "source": [
    "**Idea:** diseñar un **esquema canónico** mínimo para consultas unificadas.\n",
    "\n",
    "- **Dimensión Territorio**: `entidad_id`, `entidad_nombre`, (opc. `municipio_id`, `municipio_nombre`)\n",
    "- **Dimensión Tiempo**: `fecha` (día), (opc. `semana`, `mes`)\n",
    "- **Hechos Clínicos (casos)**: variables de `COVID19MEXICO.csv` limpias y enlazadas con catálogos.\n",
    "- **Hechos Serie**: `(entidad_id, fecha, metrica, valor)` donde `metrica ∈ {confirmados, defunciones, negativos, sospechosos}`\n",
    "- **Hechos Texto (vectorial)**: `(doc_id, embedding, metadatos)`\n",
    "\n",
    "**Campos de enlace (join keys):**\n",
    "- `ENTIDAD_RES` (relacional) ↔ `cve_ent` (series) ↔ `CLAVE_ENTIDAD` (catálogo ENTIDADES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c5a4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-17T22:52:40.637884Z",
     "start_time": "2025-09-17T22:52:37.776896Z"
    }
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# Initialize DuckDB connection\n",
    "con = duckdb.connect(database=':memory:')\n",
    "\n",
    "# Fix the date parsing in the load_and_prepare_data function\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare all required datasets with proper column handling\"\"\"\n",
    "    data_dir = \"../data\"\n",
    "    \n",
    "    # Load COVID cases data (relational) - all yearly CSV files\n",
    "    relational_dir = os.path.join(data_dir, \"relational\")\n",
    "    csv_files = [\n",
    "        \"COVID19MEXICO2020.csv\",\n",
    "        \"COVID19MEXICO2021.csv\", \n",
    "        \"COVID19MEXICO2022.csv\",\n",
    "        \"COVID19MEXICO2023.csv\"\n",
    "    ]\n",
    "    \n",
    "    df_cases_list = []\n",
    "    all_columns = set()\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(relational_dir, csv_file)\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                # Fix DtypeWarning by specifying dtype for problematic column\n",
    "                df_year = pd.read_csv(file_path, dtype={'PAIS_ORIGEN': str})\n",
    "                df_year['YEAR'] = csv_file.replace('COVID19MEXICO', '').replace('.csv', '')\n",
    "                df_cases_list.append(df_year)\n",
    "                all_columns.update(df_year.columns)\n",
    "                print(f\"✓ Loaded {csv_file}: {df_year.shape}\")\n",
    "                print(f\"  Columns: {list(df_year.columns)}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠ Error loading {csv_file}: {e}\")\n",
    "        else:\n",
    "            print(f\"⚠ File not found: {csv_file}\")\n",
    "    \n",
    "    # Combine all yearly data\n",
    "    if df_cases_list:\n",
    "        df_cases = pd.concat(df_cases_list, ignore_index=True)\n",
    "        print(f\"✓ Combined relational data: {df_cases.shape}\")\n",
    "        print(f\"✓ Years included: {sorted(df_cases['YEAR'].unique())}\")\n",
    "        print(f\"✓ All unique columns: {sorted(all_columns)}\")\n",
    "        \n",
    "        # Check for key columns\n",
    "        key_columns = ['CLASIFICACION_FINAL', 'RESULTADO_LAB', 'FECHA_ACTUALIZACION', 'ENTIDAD_RES']\n",
    "        missing_cols = [col for col in key_columns if col not in df_cases.columns]\n",
    "        if missing_cols:\n",
    "            print(f\"⚠ Missing key columns: {missing_cols}\")\n",
    "        else:\n",
    "            print(f\"✓ All key columns present: {key_columns}\")\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"No CSV files could be loaded\")\n",
    "    \n",
    "    # Load catalog data\n",
    "    catalogos_file = os.path.join(data_dir, \"relational/240708 Catalogos.xlsx\")\n",
    "    cats = pd.read_excel(catalogos_file, sheet_name=None)\n",
    "    \n",
    "    # Load and transform series data (graph)\n",
    "    graph_dir = os.path.join(data_dir, \"graph\")\n",
    "    series_files = {\n",
    "        'confirmados': 'Casos_Diarios_Estado_Nacional_Confirmados_20230625.csv',\n",
    "        'defunciones': 'Casos_Diarios_Estado_Nacional_Defunciones_20230625.csv',\n",
    "        'negativos': 'Casos_Diarios_Estado_Nacional_Negativos_20230625.csv',\n",
    "        'sospechosos': 'Casos_Diarios_Estado_Nacional_Sospechosos_20230625.csv'\n",
    "    }\n",
    "    \n",
    "    series_long = {}\n",
    "    for name, filename in series_files.items():\n",
    "        file_path = os.path.join(graph_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Transform wide format to long format\n",
    "        id_vars = ['cve_ent', 'poblacion', 'nombre']\n",
    "        date_cols = [col for col in df.columns if col not in id_vars]\n",
    "        \n",
    "        df_long = pd.melt(\n",
    "            df, \n",
    "            id_vars=id_vars, \n",
    "            value_vars=date_cols,\n",
    "            var_name='fecha', \n",
    "            value_name='valor'\n",
    "        )\n",
    "        df_long['metrica'] = name\n",
    "        # FIX: Parse dates with correct format (DD-MM-YYYY)\n",
    "        df_long['fecha'] = pd.to_datetime(df_long['fecha'], format='%d-%m-%Y')\n",
    "        \n",
    "        series_long[name] = df_long\n",
    "    \n",
    "    print(f\"✓ Loaded graph data: {len(series_long)} series\")\n",
    "    \n",
    "    # Load text data (Twitter sentiment) - sample for analysis\n",
    "    text_dir = os.path.join(data_dir, \"text\")\n",
    "    twitter_files = [\"mexico_1-003.tsv\", \"mexico_2-001.tsv\", \"mexico_3-004.tsv\"]\n",
    "    \n",
    "    # Load a sample of Twitter data for sentiment analysis\n",
    "    df_twitter_sample = None\n",
    "    for twitter_file in twitter_files[:1]:  # Use first file for sample\n",
    "        file_path = os.path.join(text_dir, twitter_file)\n",
    "        try:\n",
    "            # Read sample of Twitter data\n",
    "            df_twitter_sample = pd.read_csv(file_path, sep='\\t', nrows=10000)\n",
    "            print(f\"✓ Loaded text data sample: {df_twitter_sample.shape}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"⚠ Could not load {twitter_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return df_cases, cats, series_long, df_twitter_sample\n",
    "\n",
    "# Load all data modalities using robust function\n",
    "try:\n",
    "    df_cases, cats, series_long, df_twitter_sample = load_and_prepare_data()\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error loading data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Register relational data\n",
    "con.register('cases_raw', df_cases)\n",
    "\n",
    "# Register graph data (series)\n",
    "for name, df in series_long.items():\n",
    "    con.register(f'series_{name}', df)\n",
    "\n",
    "# Register text data if available\n",
    "if df_twitter_sample is not None:\n",
    "    con.register('twitter_sample', df_twitter_sample)\n",
    "\n",
    "# Prepare entity catalog\n",
    "cat_ent = cats.get('Catálogo de ENTIDADES')\n",
    "if cat_ent is not None:\n",
    "    cat_ent = cat_ent.rename(columns={\n",
    "        'CLAVE_ENTIDAD': 'entidad_id',\n",
    "        'ENTIDAD_FEDERATIVA': 'entidad_nombre', \n",
    "        'ABREVIATURA': 'ent_abbr'\n",
    "    })\n",
    "    con.register('cat_entidades', cat_ent)\n",
    "\n",
    "# Create unified series view\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW series_all AS\n",
    "SELECT \n",
    "    cve_ent AS entidad_id,\n",
    "    nombre AS entidad_nombre, \n",
    "    poblacion,\n",
    "    fecha,\n",
    "    metrica,\n",
    "    valor\n",
    "FROM (\n",
    "    SELECT cve_ent, poblacion, nombre, fecha, metrica, valor FROM series_confirmados\n",
    "    UNION ALL\n",
    "    SELECT cve_ent, poblacion, nombre, fecha, metrica, valor FROM series_defunciones\n",
    "    UNION ALL\n",
    "    SELECT cve_ent, poblacion, nombre, fecha, metrica, valor FROM series_negativos\n",
    "    UNION ALL\n",
    "    SELECT cve_ent, poblacion, nombre, fecha, metrica, valor FROM series_sospechosos\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# MULTI-MODAL ANALYSIS PIPELINE\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPREHENSIVE COVID-19 MULTI-MODAL ANALYSIS PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. RELATIONAL-GRAPH INTEGRATION: Match by date and location\n",
    "print(\"\\n1. RELATIONAL-GRAPH INTEGRATION:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# First, let's check the data types and fix date columns\n",
    "con.execute(\"\"\"\n",
    "-- Convert date columns to proper date format\n",
    "CREATE OR REPLACE VIEW cases_raw_processed AS\n",
    "SELECT \n",
    "    *,\n",
    "    CAST(FECHA_ACTUALIZACION AS DATE) as case_date_clean\n",
    "FROM cases_raw\n",
    "WHERE FECHA_ACTUALIZACION IS NOT NULL\n",
    "    AND FECHA_ACTUALIZACION != '9999-99-99'  -- Filter out invalid dates\n",
    "    AND CAST(SUBSTR(FECHA_ACTUALIZACION, 1, 4) AS INTEGER) BETWEEN 2020 AND 2023;  -- Filter valid year range\n",
    "\"\"\")\n",
    "\n",
    "con.execute(\"\"\"\n",
    "-- Convert series dates to proper date format\n",
    "CREATE OR REPLACE VIEW series_all_processed AS\n",
    "SELECT \n",
    "    *,\n",
    "    CAST(fecha AS DATE) as series_date_clean\n",
    "FROM series_all\n",
    "WHERE fecha IS NOT NULL;\n",
    "\"\"\")\n",
    "\n",
    "# Memory-efficient integration with chunked processing\n",
    "print(\"Creating memory-efficient relational-graph integration...\")\n",
    "\n",
    "# Set conservative memory parameters\n",
    "con.execute(\"SET memory_limit='16GB'\")  # Reduced from 32GB\n",
    "con.execute(\"SET max_temp_directory_size='100GB'\")  # Increased temp space\n",
    "con.execute(\"SET threads=2\")  # Reduced threads to save memory\n",
    "con.execute(\"SET preserve_insertion_order=false\")\n",
    "\n",
    "# First, create an empty table with the right structure\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE TABLE relational_graph_integration (\n",
    "    case_date DATE,\n",
    "    entidad_id INTEGER,\n",
    "    entidad_nombre VARCHAR,\n",
    "    CLASIFICACION_FINAL INTEGER,\n",
    "    RESULTADO_LAB INTEGER,\n",
    "    series_date DATE,\n",
    "    metrica VARCHAR,\n",
    "    series_value DOUBLE,\n",
    "    date_diff_days DOUBLE\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Process by year and entity to reduce memory usage\n",
    "years = [2020, 2021, 2022, 2023]\n",
    "\n",
    "for year in years:\n",
    "    print(f\"Processing year {year}...\")\n",
    "    \n",
    "    # Get entities for this year to process in smaller chunks\n",
    "    entities = con.execute(f\"\"\"\n",
    "        SELECT DISTINCT c.ENTIDAD_RES \n",
    "        FROM cases_raw_processed c\n",
    "        WHERE c.FECHA_ACTUALIZACION IS NOT NULL\n",
    "            AND EXTRACT(YEAR FROM c.case_date_clean) = {year}\n",
    "        ORDER BY c.ENTIDAD_RES\n",
    "    \"\"\").fetchall()\n",
    "    \n",
    "    print(f\"  Processing {len(entities)} entities for year {year}\")\n",
    "    \n",
    "    # Process entities in batches of 5 to reduce memory usage\n",
    "    batch_size = 5\n",
    "    for i in range(0, len(entities), batch_size):\n",
    "        entity_batch = entities[i:i+batch_size]\n",
    "        entity_list = [str(e[0]) for e in entity_batch]\n",
    "        entity_filter = f\"c.ENTIDAD_RES IN ({','.join(entity_list)})\"\n",
    "        \n",
    "        print(f\"    Processing entities {i+1}-{min(i+batch_size, len(entities))} of {len(entities)}\")\n",
    "        \n",
    "        con.execute(f\"\"\"\n",
    "        INSERT INTO relational_graph_integration\n",
    "        SELECT \n",
    "            c.FECHA_ACTUALIZACION as case_date,\n",
    "            c.ENTIDAD_RES as entidad_id,\n",
    "            e.entidad_nombre,\n",
    "            c.CLASIFICACION_FINAL,\n",
    "            c.RESULTADO_LAB,\n",
    "            s.fecha as series_date,\n",
    "            s.metrica,\n",
    "            s.valor as series_value,\n",
    "            ABS(julian(c.case_date_clean) - julian(s.series_date_clean)) as date_diff_days\n",
    "        FROM cases_raw_processed c\n",
    "        LEFT JOIN cat_entidades e ON c.ENTIDAD_RES = e.entidad_id\n",
    "        LEFT JOIN series_all_processed s ON c.ENTIDAD_RES = s.entidad_id \n",
    "            AND ABS(julian(c.case_date_clean) - julian(s.series_date_clean)) <= 7\n",
    "        WHERE c.FECHA_ACTUALIZACION IS NOT NULL\n",
    "            AND s.fecha IS NOT NULL\n",
    "            AND EXTRACT(YEAR FROM c.case_date_clean) = {year}\n",
    "            AND {entity_filter}\n",
    "        ORDER BY c.FECHA_ACTUALIZACION, c.ENTIDAD_RES, s.metrica;\n",
    "        \"\"\")\n",
    "    \n",
    "    print(f\"Completed year {year}\")\n",
    "\n",
    "# Show final results\n",
    "integration_count = con.execute('SELECT COUNT(*) FROM relational_graph_integration').fetchone()[0]\n",
    "print(f\"Total integration records: {integration_count:,}\")\n",
    "\n",
    "# 2. GRAPH-TEXT INTEGRATION: Correlate sentiment with case numbers\n",
    "print(\"\\n2. GRAPH-TEXT INTEGRATION (Sentiment-Case Correlation):\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "if df_twitter_sample is not None:\n",
    "    # Process Twitter data for sentiment analysis\n",
    "    con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW twitter_processed AS\n",
    "    SELECT \n",
    "        tweet_id,\n",
    "        date_time,\n",
    "        lang,\n",
    "        sentiment_label,\n",
    "        sentiment_conf,\n",
    "        CASE \n",
    "            WHEN sentiment_label = 1 THEN 'positive'\n",
    "            WHEN sentiment_label = 0 THEN 'neutral'\n",
    "            WHEN sentiment_label = -1 THEN 'negative'\n",
    "            ELSE 'unknown'\n",
    "        END as sentiment_category,\n",
    "        CAST(SUBSTR(date_time, 1, 10) AS DATE) as tweet_date\n",
    "    FROM twitter_sample\n",
    "    WHERE sentiment_label IS NOT NULL\n",
    "        AND date_time IS NOT NULL;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create daily sentiment aggregation\n",
    "    con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW daily_sentiment AS\n",
    "    SELECT \n",
    "        tweet_date,\n",
    "        sentiment_category,\n",
    "        COUNT(*) as tweet_count,\n",
    "        AVG(sentiment_conf) as avg_confidence\n",
    "    FROM twitter_processed\n",
    "    GROUP BY tweet_date, sentiment_category\n",
    "    ORDER BY tweet_date, sentiment_category;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Correlate sentiment with case numbers\n",
    "    con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW sentiment_case_correlation AS\n",
    "    SELECT \n",
    "        s.fecha,\n",
    "        s.entidad_id,\n",
    "        s.entidad_nombre,\n",
    "        s.metrica,\n",
    "        s.valor as case_count,\n",
    "        ts.sentiment_category,\n",
    "        ts.tweet_count as sentiment_tweets,\n",
    "        ts.avg_confidence,\n",
    "        ABS(julian(s.fecha) - julian(ts.tweet_date)) as date_diff_days\n",
    "    FROM series_all s\n",
    "    LEFT JOIN daily_sentiment ts ON ABS(julian(s.fecha) - julian(ts.tweet_date)) <= 3\n",
    "    WHERE s.metrica IN ('confirmados', 'defunciones')\n",
    "        AND ts.sentiment_category IS NOT NULL\n",
    "    ORDER BY s.fecha, s.entidad_id, s.metrica;\n",
    "    \"\"\")\n",
    "    \n",
    "    sentiment_count = con.execute('SELECT COUNT(*) FROM sentiment_case_correlation').fetchone()[0]\n",
    "    print(f\"✓ Created sentiment-case correlation: {sentiment_count:,} records\")\n",
    "    \n",
    "    # Show sentiment analysis sample\n",
    "    print(\"\\nSample sentiment analysis:\")\n",
    "    sample_sentiment = con.execute(\"\"\"\n",
    "        SELECT fecha, entidad_nombre, metrica, case_count, \n",
    "               sentiment_category, sentiment_tweets, avg_confidence\n",
    "        FROM sentiment_case_correlation \n",
    "        WHERE sentiment_category IS NOT NULL\n",
    "        ORDER BY fecha DESC, case_count DESC\n",
    "        LIMIT 5\n",
    "    \"\"\").fetchall()\n",
    "    \n",
    "    for row in sample_sentiment:\n",
    "        print(f\"  {row[0]} | {row[1]} | {row[2]} | {row[3]} | {row[4]} | {row[5]} | {row[6]:.2f}\")\n",
    "\n",
    "else:\n",
    "    print(\"⚠ Twitter data not available for sentiment analysis\")\n",
    "\n",
    "# 3. COMPREHENSIVE ANALYSIS VIEWS\n",
    "print(\"\\n3. COMPREHENSIVE ANALYSIS VIEWS:\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Create comprehensive COVID-19 analysis view\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW covid_comprehensive_analysis AS\n",
    "SELECT \n",
    "    s.fecha,\n",
    "    s.entidad_id,\n",
    "    s.entidad_nombre,\n",
    "    s.poblacion,\n",
    "    -- Case metrics\n",
    "    MAX(CASE WHEN s.metrica = 'confirmados' THEN s.valor ELSE 0 END) as casos_confirmados,\n",
    "    MAX(CASE WHEN s.metrica = 'defunciones' THEN s.valor ELSE 0 END) as defunciones,\n",
    "    MAX(CASE WHEN s.metrica = 'negativos' THEN s.valor ELSE 0 END) as casos_negativos,\n",
    "    MAX(CASE WHEN s.metrica = 'sospechosos' THEN s.valor ELSE 0 END) as casos_sospechosos,\n",
    "    -- Calculated metrics\n",
    "    MAX(CASE WHEN s.metrica = 'confirmados' THEN s.valor ELSE 0 END) * 100000.0 / s.poblacion as tasa_confirmados_100k,\n",
    "    MAX(CASE WHEN s.metrica = 'defunciones' THEN s.valor ELSE 0 END) * 100000.0 / s.poblacion as tasa_defunciones_100k,\n",
    "    -- Sentiment metrics (if available)\n",
    "    ts_neg.avg_confidence as avg_negative_sentiment_conf,\n",
    "    ts_pos.avg_confidence as avg_positive_sentiment_conf,\n",
    "    ts_neg.tweet_count as negative_tweets,\n",
    "    ts_pos.tweet_count as positive_tweets\n",
    "FROM series_all s\n",
    "LEFT JOIN daily_sentiment ts_neg ON ABS(julian(s.fecha) - julian(ts_neg.tweet_date)) <= 3 \n",
    "    AND ts_neg.sentiment_category = 'negative'\n",
    "LEFT JOIN daily_sentiment ts_pos ON ABS(julian(s.fecha) - julian(ts_pos.tweet_date)) <= 3 \n",
    "    AND ts_pos.sentiment_category = 'positive'\n",
    "GROUP BY s.fecha, s.entidad_id, s.entidad_nombre, s.poblacion, \n",
    "         ts_neg.avg_confidence, ts_pos.avg_confidence, \n",
    "         ts_neg.tweet_count, ts_pos.tweet_count\n",
    "ORDER BY s.fecha DESC, s.entidad_id;\n",
    "\"\"\")\n",
    "\n",
    "# Create research-ready dataset\n",
    "con.execute(\"\"\"\n",
    "CREATE OR REPLACE VIEW research_dataset AS\n",
    "SELECT \n",
    "    fecha,\n",
    "    entidad_id,\n",
    "    entidad_nombre,\n",
    "    poblacion,\n",
    "    casos_confirmados,\n",
    "    defunciones,\n",
    "    casos_negativos,\n",
    "    casos_sospechosos,\n",
    "    tasa_confirmados_100k,\n",
    "    tasa_defunciones_100k,\n",
    "    -- Sentiment features\n",
    "    COALESCE(avg_negative_sentiment_conf, 0) as negative_sentiment_confidence,\n",
    "    COALESCE(avg_positive_sentiment_conf, 0) as positive_sentiment_confidence,\n",
    "    COALESCE(negative_tweets, 0) as negative_tweet_count,\n",
    "    COALESCE(positive_tweets, 0) as positive_tweet_count,\n",
    "    -- Derived features\n",
    "    CASE WHEN casos_confirmados > 0 THEN defunciones * 100.0 / casos_confirmados ELSE 0 END as case_fatality_rate,\n",
    "    CASE WHEN poblacion > 0 THEN (casos_confirmados + casos_negativos) * 100.0 / poblacion ELSE 0 END as test_positivity_rate\n",
    "FROM covid_comprehensive_analysis\n",
    "WHERE casos_confirmados > 0 OR defunciones > 0;\n",
    "\"\"\")\n",
    "\n",
    "print(\"✓ Created comprehensive analysis views:\")\n",
    "print(\"  - covid_comprehensive_analysis: Multi-modal integrated data\")\n",
    "print(\"  - research_dataset: Clean dataset for research\")\n",
    "\n",
    "# 4. ANALYSIS INSIGHTS\n",
    "print(\"\\n4. ANALYSIS INSIGHTS:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Top affected states\n",
    "print(\"\\nTop 10 states by total confirmed cases:\")\n",
    "top_states = con.execute(\"\"\"\n",
    "    SELECT entidad_nombre, SUM(casos_confirmados) as total_cases, \n",
    "           SUM(defunciones) as total_deaths,\n",
    "           AVG(tasa_confirmados_100k) as avg_rate_100k\n",
    "    FROM research_dataset \n",
    "    GROUP BY entidad_id, entidad_nombre\n",
    "    ORDER BY total_cases DESC\n",
    "    LIMIT 10\n",
    "\"\"\").fetchall()\n",
    "\n",
    "for i, row in enumerate(top_states, 1):\n",
    "    print(f\"  {i:2d}. {row[0]:<25} | Cases: {row[1]:>8,} | Deaths: {row[2]:>6,} | Rate: {row[3]:>6.1f}\")\n",
    "\n",
    "# Fixed sentiment analysis with proper column handling\n",
    "if df_twitter_sample is not None:\n",
    "    # Process Twitter data for sentiment analysis with proper date parsing\n",
    "    con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW twitter_processed AS\n",
    "    SELECT \n",
    "        tweet_id,\n",
    "        CAST(date_time AS TIMESTAMP) as tweet_timestamp,\n",
    "        DATE(CAST(date_time AS TIMESTAMP)) as tweet_date,\n",
    "        lang,\n",
    "        sentiment_label,\n",
    "        sentiment_conf,\n",
    "        CASE \n",
    "            WHEN sentiment_label = -1 THEN 'negative'\n",
    "            WHEN sentiment_label = 0 THEN 'neutral' \n",
    "            WHEN sentiment_label = 1 THEN 'positive'\n",
    "            ELSE 'unknown'\n",
    "        END as sentiment_category,\n",
    "        -- Extract location information for better correlation\n",
    "        COALESCE(geo_state, place_state, user_loc_state) as tweet_state,\n",
    "        COALESCE(geo_country_code, place_country_code, user_loc_country_code) as tweet_country\n",
    "    FROM twitter_sample\n",
    "    WHERE sentiment_label IS NOT NULL \n",
    "        AND sentiment_conf IS NOT NULL\n",
    "        AND date_time IS NOT NULL\n",
    "        AND (geo_country_code = 'MX' OR place_country_code = 'MX' OR user_loc_country_code = 'MX');\n",
    "    \"\"\")\n",
    "    \n",
    "    # Create daily sentiment aggregation with location filtering\n",
    "    con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW daily_sentiment AS\n",
    "    SELECT \n",
    "        tweet_date,\n",
    "        sentiment_category,\n",
    "        COUNT(*) as tweet_count,\n",
    "        AVG(sentiment_conf) as avg_confidence\n",
    "    FROM twitter_processed\n",
    "    WHERE tweet_country = 'MX'  -- Only Mexican tweets\n",
    "    GROUP BY tweet_date, sentiment_category\n",
    "    ORDER BY tweet_date, sentiment_category;\n",
    "    \"\"\")\n",
    "    \n",
    "    # Improved sentiment-case correlation with better date matching\n",
    "    con.execute(\"\"\"\n",
    "    CREATE OR REPLACE VIEW sentiment_case_correlation AS\n",
    "    SELECT \n",
    "        s.fecha,\n",
    "        s.entidad_id,\n",
    "        s.entidad_nombre,\n",
    "        s.metrica,\n",
    "        s.valor as case_count,\n",
    "        ts.sentiment_category,\n",
    "        ts.tweet_count as sentiment_tweets,\n",
    "        ts.avg_confidence,\n",
    "        ABS(julian(s.fecha) - julian(ts.tweet_date)) as date_diff_days\n",
    "    FROM series_all s\n",
    "    LEFT JOIN daily_sentiment ts ON ABS(julian(s.fecha) - julian(ts.tweet_date)) <= 3\n",
    "    WHERE s.metrica IN ('confirmados', 'defunciones')\n",
    "        AND ts.sentiment_category IS NOT NULL\n",
    "    ORDER BY s.fecha, s.entidad_id, s.metrica;\n",
    "    \"\"\")\n",
    "    \n",
    "    sentiment_corr = con.execute(\"\"\"\n",
    "        SELECT \n",
    "            entidad_nombre,\n",
    "            metrica,\n",
    "            COUNT(*) as count,\n",
    "            AVG(case_count) as avg_cases,\n",
    "            AVG(sentiment_tweets) as avg_tweets,\n",
    "            AVG(avg_confidence) as avg_conf\n",
    "        FROM sentiment_case_correlation \n",
    "        WHERE sentiment_category IS NOT NULL\n",
    "        GROUP BY entidad_nombre, metrica\n",
    "        ORDER BY entidad_nombre, metrica\n",
    "    \"\"\").fetchall()\n",
    "\n",
    "    for row in sentiment_corr:\n",
    "        print(f\"  {row[0]:<12} | {row[1]:<8} | Count: {row[2]:>6} | Cases: {row[3]:>6.1f} | Tweets: {row[4]:>6.1f} | Conf: {row[5]:>5.2f}\")\n",
    "\n",
    "# 5. EXPORT FOR RESEARCH\n",
    "print(\"\\n5. RESEARCH DATASET EXPORT:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Get dataset statistics\n",
    "dataset_stats = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(DISTINCT entidad_id) as unique_states,\n",
    "        MIN(fecha) as start_date,\n",
    "        MAX(fecha) as end_date,\n",
    "        SUM(casos_confirmados) as total_cases,\n",
    "        SUM(defunciones) as total_deaths\n",
    "    FROM research_dataset\n",
    "\"\"\").fetchone()\n",
    "\n",
    "# Get year distribution from the combined cases data\n",
    "year_stats = con.execute(\"\"\"\n",
    "    SELECT \n",
    "        YEAR,\n",
    "        COUNT(*) as records,\n",
    "        COUNT(DISTINCT ENTIDAD_RES) as unique_entities\n",
    "    FROM cases_raw\n",
    "    GROUP BY YEAR\n",
    "    ORDER BY YEAR\n",
    "\"\"\").fetchall()\n",
    "\n",
    "print(f\"✓ Research dataset ready:\")\n",
    "print(f\"  - Records: {dataset_stats[0]:,}\")\n",
    "print(f\"  - States: {dataset_stats[1]}\")\n",
    "print(f\"  - Date range: {dataset_stats[2]} to {dataset_stats[3]}\")\n",
    "print(f\"  - Total cases: {dataset_stats[4]:,}\")\n",
    "print(f\"  - Total deaths: {dataset_stats[5]:,}\")\n",
    "\n",
    "print(f\"\\n✓ Multi-year data distribution:\")\n",
    "for year, records, entities in year_stats:\n",
    "    print(f\"  - {year}: {records:,} records, {entities} entities\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MULTI-MODAL COVID-19 ANALYSIS PIPELINE COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nAvailable views for analysis:\")\n",
    "print(\"  - series_all: Unified time series data\")\n",
    "print(\"  - relational_graph_integration: Relational + Graph data\")\n",
    "print(\"  - sentiment_case_correlation: Graph + Text sentiment data\")\n",
    "print(\"  - covid_comprehensive_analysis: All modalities integrated\")\n",
    "print(\"  - research_dataset: Clean dataset for research\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff024f4e",
   "metadata": {},
   "source": [
    "### Problemática por mala toma de decisiones derivada de datos de baja calidad\n",
    "\n",
    "Durante la pandemia de COVID-19 en México, decisiones críticas (como asignación de recursos hospitalarios, distribución de vacunas y diseño de campañas de prevención) pudieron verse afectadas por datos de baja calidad.\n",
    "\n",
    "Dimensiones de calidad afectadas:\n",
    "\n",
    "- **Completitud**: valores faltantes en `FECHA_DEF`, `UCI`, `INTUBADO`, lo que impide medir correctamente la mortalidad y severidad.\n",
    "\n",
    "- **Consistencia**: `RESULTADO_LAB` vs `CLASIFICACION_FINAL` no siempre coinciden (ej. casos confirmados sin prueba positiva).\n",
    "\n",
    "- **Exactitud**: errores de captura en `EDAD` (valores extremos como 999 años).\n",
    "\n",
    "- **Oportunidad**: retraso en `FECHA_ACTUALIZACION` genera reportes desfasados que afectan la toma de decisiones en tiempo real.\n",
    "\n",
    "- **Validez**: códigos de entidades (`ENTIDAD_RES`) no válidos que generan casos mal asignados a estados inexistentes.\n",
    "\n",
    "Consecuencia:\n",
    "Si los responsables asignan camas UCI en base a reportes incompletos o tardíos, se puede subestimar la demanda real, provocando saturación en hospitales de ciertos estados mientras otros quedan con recursos sin usar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b434fcf1",
   "metadata": {},
   "source": [
    "### Preguntas descriptivas (qué ha pasado, qué está pasando):\n",
    "\n",
    "- ¿Cuál fue la evolución del número de casos confirmados por año y por estado (`YEAR`, `ENTIDAD_RES`, `CLASIFICACION_FINAL`)?\n",
    "\n",
    "- ¿Qué proporción de casos confirmados resultaron en hospitalización (`TIPO_PACIENTE`)?\n",
    "\n",
    "- ¿Cuál es la distribución de comorbilidades (`DIABETES`, `OBESIDAD`, `HIPERTENSION`) entre los casos positivos?\n",
    "\n",
    "- ¿Cuál es la tasa de letalidad por entidad (`ENTIDAD_RES`) y grupo de edad (`EDAD`)?\n",
    "\n",
    "- ¿Cuántos pacientes intubados (`INTUBADO`) hubo en cada año?\n",
    "\n",
    "- ¿Qué sectores de salud (`SECTOR`) concentraron más casos confirmados y defunciones?\n",
    "\n",
    "- ¿Cuál fue la distribución de casos en población indígena (`HABLA_LENGUA_INDIG`, `INDIGENA`)?\n",
    "\n",
    "- ¿Cuántos casos positivos corresponden a mujeres embarazadas (`EMBARAZO`)?\n",
    "\n",
    "- ¿Qué municipios (`MUNICIPIO_RES`) tuvieron mayor número de casos por entidad?\n",
    "\n",
    "- ¿Cuál fue la proporción de resultados positivos en pruebas de laboratorio vs antígeno (`RESULTADO_LAB`, `RESULTADO_ANTIGENO`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8491d8",
   "metadata": {},
   "source": [
    "### Preguntas predictivas (qué podría pasar, escenarios futuros):\n",
    "\n",
    "- ¿Cuál es la probabilidad de que un paciente con ciertas comorbilidades (ej. `DIABETES`, `OBESIDAD`, `HIPERTENSION`) requiera UCI (`UCI`)?\n",
    "\n",
    "- ¿Qué factores aumentan el riesgo de defunción (`FECHA_DEF`) en pacientes hospitalizados (`TIPO_PACIENTE`)?\n",
    "\n",
    "- ¿Qué estados (`ENTIDAD_RES`) tienen mayor probabilidad de repunte en casos durante la siguiente temporada invernal?\n",
    "\n",
    "- ¿Cuál es la probabilidad de hospitalización según la edad (`EDAD`) y sexo (`SEXO`)?\n",
    "\n",
    "- ¿Qué características clínicas predicen la necesidad de intubación (`INTUBADO`)?\n",
    "\n",
    "- ¿Cuál es la probabilidad de que un paciente migrante (`MIGRANTE`) sea hospitalizado?\n",
    "\n",
    "- ¿Qué combinación de factores predice mayor mortalidad en mujeres embarazadas (`EMBARAZO` + `comorbilidades`)?\n",
    "\n",
    "- ¿Cuál es la probabilidad de positividad de una prueba de antígeno según entidad y fecha (`RESULTADO_ANTIGENO` + `FECHA_INGRESO`)?\n",
    "\n",
    "- ¿Qué municipios tienen mayor riesgo de saturación hospitalaria en caso de un nuevo brote?\n",
    "\n",
    "- ¿Qué relación hay entre sentimientos expresados en redes (text data) y repunte de casos (sentiment_case_correlation)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050fa9c1",
   "metadata": {},
   "source": [
    "### Objetos relevantes para la toma de decisiones\n",
    "\n",
    "Campos clave del dataset relacional (estructurados):\n",
    "\n",
    "- **Identificación y tiempo**: `ID_REGISTRO`, `FECHA_ACTUALIZACION`, `FECHA_INGRESO`, `FECHA_SINTOMAS`, `FECHA_DEF`, `YEAR`.\n",
    "\n",
    "- **Ubicación**: `ENTIDAD_RES`, `MUNICIPIO_RES`, `ENTIDAD_UM`.\n",
    "\n",
    "- **Datos demográficos**: `SEXO`, `EDAD`, `NACIONALIDAD`, `EMBARAZO`, `HABLA_LENGUA_INDIG`, `INDIGENA`.\n",
    "\n",
    "- **Estado clínico**: `TIPO_PACIENTE`, `INTUBADO`, `NEUMONIA`, `UCI`.\n",
    "\n",
    "- **Comorbilidades**: `DIABETES`, `OBESIDAD`, `HIPERTENSION`, `RENAL_CRONICA`, `ASMA`, `EPOC`, `CARDIOVASCULAR`, `INMUSUPR`.\n",
    "\n",
    "- **Resultados de pruebas**: `RESULTADO_LAB`, `RESULTADO_ANTIGENO`, `CLASIFICACION_FINAL`.\n",
    "\n",
    "- **Factores externos**: `MIGRANTE`, `PAIS_ORIGEN`, `SECTOR`.\n",
    "\n",
    "Nodos del grafo (series temporales):\n",
    "\n",
    "- Relaciones entre `ENTIDAD_RES` y evolución temporal de casos.\n",
    "\n",
    "- Nodos representando cadenas de contagio o correlaciones por municipio.\n",
    "\n",
    "Texto (no estructurado):\n",
    "\n",
    "- Palabras clave en reportes/noticias con sentimientos (ej. “saturación”, “rebrote”, “vacunación”).\n",
    "\n",
    "- Polaridad y subjetividad como features de predicción."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
